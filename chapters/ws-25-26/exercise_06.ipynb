{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d0cd2c-e16e-4b61-abb1-991cee3165b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "authors:\n",
    "  - name: Tom Siegl\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330f2c4-c934-49ef-ab7c-f5e66c0b7703",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "# 06: Regularization and Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745bb59-84f0-4159-bf9f-38c0f4395538",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685b94c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "In last week's exercise, we have seen, that the closed-form solution to $\\hat{\\beta}$ fails, when variables are perfectly correlated, because the inverse we are trying to compute does not exist.\n",
    "Let's look at what happens, when we have two variables, that are not linearly dependent, but very well correlated.\n",
    "\n",
    "We first generate some data like in the lecture.\n",
    "This data has two independent variables $x_1, x_2$ and one dependent variable $y$.\n",
    "The independent variables are sampled on a straight line $x_1 = x_2$ plus a small amount of noise.\n",
    "The dependent variable is computed for these sampled points from a linear model plus a small amount of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98494dc-3da5-46e4-a119-f076c32f3d80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell contains all imports for the entire notebook.\n",
    "# Take this as an overview of the libraries we use today.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1682c79-13a6-4a3b-8765-2d4085152f42",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def linear_model(X, beta):\n",
    "    return np.stack((np.ones(X.shape[0]), *(X.T)), axis=-1) @ beta\n",
    "\n",
    "\n",
    "def generate_data(orig_beta, num_points):\n",
    "    x_noise_std = 1e-3\n",
    "    y_noise_std = 1e-2\n",
    "    \n",
    "    xs = np.stack((np.linspace(-10, 10, num_points), np.linspace(-10, 10, num_points)), axis=-1)\n",
    "    xs += np.stack((np.random.normal(0, x_noise_std, num_points), np.random.normal(0, x_noise_std, num_points)), axis=-1)\n",
    "    \n",
    "    ys = linear_model(xs, model1_beta)\n",
    "    ys += np.random.normal(0, y_noise_std, (num_points,1))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xs)\n",
    "    xs = scaler.transform(xs)\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "def plot_data_2d(xs, ys):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    scatter = axs[0].scatter(xs[:, 0], xs[:, 1], c=ys)\n",
    "    plt.colorbar(scatter, ax=axs[0], label=\"$y$\" )\n",
    "    axs[0].set_xlabel(\"$x_1$\")\n",
    "    axs[0].set_ylabel(\"$x_2$\")\n",
    "    axs[0].set_title(\"$y$ vs. ($x_1$, $x_2$)\")\n",
    "    \n",
    "    axs[1].scatter(xs[:, 1], ys)\n",
    "    axs[1].set_xlabel(\"$x_1$\")\n",
    "    axs[1].set_ylabel(\"$y$\")\n",
    "    axs[1].set_title(\"$y$ vs. $x_1$\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a14f15-e07a-48d3-8a21-b85ce72af6b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "model1_beta = np.array([[5],  # beta0\n",
    "                        [1],  # beta1\n",
    "                        [1]]) # beta2\n",
    "num_points = 101\n",
    "\n",
    "xs, ys = generate_data(model1_beta, num_points)\n",
    "plot_data_2d(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26310b-e468-433d-bcd3-7edab2ab8851",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Guess a $y$-value at the point $(x_1, x_2) = (0, 1)$ based on the plots.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c7ffc-1fe4-4c10-b267-c5b7617632f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfef285-b223-4476-afbd-d41775d20722",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Do you have a solution to the task? Then let's have a look at what linear regression has to say about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d49ca5-601a-43a4-84fa-b31a19121eed",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_predictions_grid(model, title, fig, ax):\n",
    "    grid_min = -10\n",
    "    grid_max = 10\n",
    "    grid_size = 20\n",
    "    grid_vals = np.linspace(grid_min, grid_max, grid_size)\n",
    "    xx, yy = np.meshgrid(grid_vals, grid_vals)\n",
    "    grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    preds = model.predict(grid_points)\n",
    "    preds = np.reshape(preds, (grid_size, grid_size))\n",
    "    \n",
    "    im = ax.imshow(preds, extent=(grid_min, grid_max, grid_max, grid_min))\n",
    "    ax.set_xlim((grid_min, grid_max))\n",
    "    ax.set_ylim((grid_min, grid_max))\n",
    "    ax.set_xlabel(\"$x_1$\")\n",
    "    ax.set_ylabel(\"$x_2$\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def draw_colorbar(cmap, fig, ax):\n",
    "    # dummy scalar mappable for the colorbar\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "    sm = matplotlib.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    \n",
    "    # draw colorbar on the given axis\n",
    "    cbar = fig.colorbar(sm, cax=ax)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels([\"low\", \"high\"])\n",
    "    cbar.set_label(\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1613ca-e024-45f7-83b3-fa0d1cc91ea6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    # resample data\n",
    "    xs, ys = generate_data(model1_beta, num_points)\n",
    "    \n",
    "    # fit model\n",
    "    ls_fit = LinearRegression().fit(xs, ys)\n",
    "    \n",
    "    # plot model predictions\n",
    "    ax = fig.add_subplot(3, 5, i+1)\n",
    "    plot_predictions_grid(ls_fit, f\"run {i+1}\", fig, ax)\n",
    "\n",
    "# plot colorbar\n",
    "ax_cbar = fig.add_subplot(3, 5, 11)\n",
    "ax_cbar.set_title(\"colorbar\")\n",
    "draw_colorbar(\"viridis\", fig, ax_cbar)\n",
    "\n",
    "# finalize plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e0e60-0741-453c-8aa6-cf3a59ae9e52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "As you can see, OLS regression is not very certain about that question either.\n",
    "The learned models appear to vary randomly by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb5dbe-8573-425a-9c4d-e0d26d914831",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. What is the source of the randomness, that results in the different plots above? (Name the code line in the cell that creates the plots.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebf20b-ac7f-41d1-b35b-4b470fcdcb1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b9249-05d0-4bd5-8ef1-f46f8ad36d28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "In the lecture, you have looked at some statistical tools for linear regression diagnostics.\n",
    "Your task will now be to use a diagnostic model summary to find the correlated variables.\n",
    "\n",
    "To prepare the challenge, the next code cell adds a couple of completely oncorrelated variables with no connection to $y$.\n",
    "Don't read that cell, because it is too easy to see the solution from the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4be9a-5c96-468d-9be7-132bf2f50c89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Based on the model summary, which $x$ variables do you suspect to be correlated and why?\n",
    "2. Given the model summary and a threshold $\\alpha = 0.05$, would you reject the null hypothesis $H_0: \\hat\\beta_0 = \\hat\\beta_1 = \\dots = \\hat\\beta_{10} = 0$?\n",
    "3. Given the model summary and a threshold $\\alpha = 0.05$, would you reject the null hypothesis $H_0: \\hat\\beta_0 = 0$?\n",
    "4. Given the model summary and a threshold $\\alpha = 0.05$, would you reject the null hypothesis $H_0: \\hat\\beta_1 = 0$?\n",
    "5. Bonus: If you test all the parameter values for their significance, then the event of a p-value being below or above your threshold $\\alpha$ changes meaning. Name a method that you could apply in order to regain control over the meaning of that event!\n",
    "6. Now uncomment the first line in the code cell that gave you the model summary and replace the underscode with the number of one of the suspicious variables. This replaces a variable with the same noise as there is in all the meaningless variables.\n",
    "7. How do you know whether you have hit the right variable?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ee408-ef75-415f-8d8e-f04c73f5bcfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. \n",
    "7. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf621cbb-2283-44d4-b472-d47cea4e65ed",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# generate new, uncorrelated x data with 10 variables\n",
    "num_variables = 10\n",
    "xs_large = np.random.multivariate_normal(np.zeros(num_variables+1), np.eye(num_variables+1), num_points)\n",
    "xs_large[:, 0] = np.ones((num_points,))  # add a constant to allow statsmodels to fit an intercept\n",
    "\n",
    "# insert correlated variables into the uncorrelated data\n",
    "xs_large[:, 3] = xs[:, 0]\n",
    "xs_large[:, 6] = xs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e396e79-f8e1-4f62-800d-19de24761efb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# xs_large[:, _] = np.random.normal(0, 1, num_points)\n",
    "\n",
    "# fit a model to the new data and show diagnostics\n",
    "sm_model = OLS(ys, xs_large)\n",
    "sm_ls_fit = sm_model.fit()\n",
    "display(HTML(sm_ls_fit.summary(alpha=0.05).as_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdad84-21b1-41dd-bb12-51a1eaa1adc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Let's go back to our two-dimensional data from the beginning and have a look at what the loss looks like when continually varying $\\hat\\beta$.\n",
    "\n",
    "Run the following code cells up to the next tasks box.\n",
    "They will give you visualizations of the least squares loss on our 2d data as well as the Euclidean and Manhattan distance metrics.\n",
    "The last one will print out the least squares loss at some specific points in $(\\hat\\beta_1, \\hat\\beta_2)$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233e6b9-e1f1-45c5-931f-9238f86a7d52",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_loss_surface(X, y, loss_func, loss_func_name, title, ax):\n",
    "    beta0 = 5.0\n",
    "    \n",
    "    # Parameter grids\n",
    "    xymin = -20\n",
    "    xymax = 20\n",
    "    beta1_vals = np.linspace(xymin, xymax, 200)\n",
    "    beta2_vals = np.linspace(xymin, xymax, 200)\n",
    "    \n",
    "    # Allocate loss matrix\n",
    "    loss_surface = np.zeros((len(beta2_vals), len(beta1_vals)))\n",
    "    \n",
    "    # Compute MSE over the grid\n",
    "    for i, b1 in enumerate(beta1_vals):\n",
    "        for j, b2 in enumerate(beta2_vals):\n",
    "            beta_vec = np.array([[beta0], [b1], [b2]])\n",
    "            y_pred = linear_model(X, beta_vec)\n",
    "            loss_surface[j, i] = loss_func(y, y_pred, beta_vec)\n",
    "    \n",
    "    # Plot\n",
    "    im = ax.imshow(loss_surface, origin='lower',\n",
    "               extent=[beta1_vals[0], beta1_vals[-1],\n",
    "                       beta2_vals[0], beta2_vals[-1]],\n",
    "               aspect='equal')\n",
    "    plt.colorbar(im, ax=ax, label=loss_func_name)\n",
    "    ax.contour(beta1_vals, beta2_vals, loss_surface, levels=np.linspace(loss_surface.min()+1, loss_surface.max(), 10), colors=\"white\")\n",
    "    ax.axhline(0, beta1_vals[0], beta1_vals[-1], color=\"red\")\n",
    "    ax.axvline(0, beta2_vals[0], beta2_vals[-1], color=\"red\")\n",
    "    \n",
    "    ax.set_xlabel(r'$\\hat\\beta_1$')\n",
    "    ax.set_ylabel(r'$\\hat\\beta_2$')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def least_squares_loss(y, y_pred, beta=None):\n",
    "    # return 0\n",
    "    return np.sum((ys - y_pred)**2)\n",
    "\n",
    "\n",
    "def ridge_term(y, y_pred, beta, alpha):\n",
    "    return alpha*np.sum(np.square(beta))\n",
    "\n",
    "\n",
    "def ridge_loss(y, y_pred, beta, alpha):\n",
    "    return least_squares_loss(y, y_pred) + ridge_term(y, y_pred, beta, alpha)\n",
    "\n",
    "\n",
    "def lasso_term(y, y_pred, beta, alpha):\n",
    "    return alpha*np.sum(np.abs(beta))\n",
    "\n",
    "\n",
    "def lasso_loss(y, y_pred, beta, alpha):\n",
    "    return least_squares_loss(y, y_pred) + lasso_term(y, y_pred, beta, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de469f-13d5-4a3d-a0a8-4556d2a0eaef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "plot_loss_surface(xs, ys, least_squares_loss, \"Least Squares Loss\", \"Least Squares Loss Surface\", ax1)\n",
    "plot_loss_surface(xs, ys, lambda y, y_pred, beta: ridge_term(y, y_pred, beta, 1), \"Distance\", \"Euclidean Distance to $(0,0)$\", ax2)\n",
    "plot_loss_surface(xs, ys, lambda y, y_pred, beta: lasso_term(y, y_pred, beta, 1), \"Distance\", \"Manhattan Distance to $(0,0)$\", ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5040f30-0c56-459c-a4ae-0f2406c910f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "beta_vec1 = np.array([[5.0], [12.0], [0.0]])\n",
    "y_pred1 = linear_model(xs, beta_vec1)\n",
    "print(\"Least Squares loss at (12, 0):\", least_squares_loss(ys, y_pred1, beta_vec1))\n",
    "\n",
    "beta_vec2 = np.array([[5.0], [0.0], [12.0]])\n",
    "y_pred2 = linear_model(xs, beta_vec2)\n",
    "print(\"Least Squares loss at (0, 12):\", least_squares_loss(ys, y_pred2, beta_vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db846c12-cdd1-435c-9171-892f48eff959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Make sure to have a close look at the point losses printed by the last code cell for answering tasks 1-3!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdd51a-2609-4f7a-b8a1-03ef07c06543",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Visually identify the optimal $\\hat\\beta_1$ and $\\hat\\beta_2$ based on the least squares loss.\n",
    "2. Visually identify the optimal $\\hat\\beta_1$ and $\\hat\\beta_2$ based on the least squares loss, while also minimizing the Euclidean distance to the coordinate center $(0, 0)$ with equal weight.\n",
    "3. Visually identify the optimal $\\hat\\beta_1$ and $\\hat\\beta_2$ based on the least squares loss, while also minimizing the Manhattan distance to the coordinate center $(0, 0)$ with equal weight.\n",
    "4. Which of the following optimization targets do tasks 1-3 correspond?\n",
    "$$\n",
    "\\arg \\min_{\\beta} \\sum_{i=1}^{p} (y_i - x_i^T\\beta)^2 + \\sum_{i=1}^{p} |\\beta_i|\n",
    "$$\n",
    "$$\n",
    "\\arg \\min_{\\beta} \\sum_{i=1}^{p} (y_i - x_i^T\\beta)^2 + \\sum_{i=1}^{p} \\beta_i^2\n",
    "$$\n",
    "$$\n",
    "\\arg \\min_{\\beta} \\sum_{i=1}^{p} (y_i - x_i^T\\beta)^2\n",
    "$$\n",
    "$$\n",
    "\\arg \\min_{\\beta} \\sum_{i=1}^{p} (y_i - x_i^T\\beta)^2 + \\alpha \\sum_{i=1}^{p} \\beta_i^2 + (1-\\alpha) \\sum_{i=1}^{p} |\\beta_i|\n",
    "$$\n",
    "5. Which regularization methods do each of the optimization targets of task 4 correspond to?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccec59-751b-495d-bfee-76a4518fc505",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81497534-1f69-4e6e-b192-7b22cc56f4ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "The following code cell gives you the computed solutions to tasks 1-3, when you know how to match the tasks to regularization methods.\n",
    "\n",
    "The code cell after that shows the predictions of the Ridge and Lasso models in $(x_1, x_2)$ space.\n",
    "Note that these are much less random compared to the OLS models you have seen at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cfb4f-be8a-43ef-bf15-8c6e909e6a84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# fit models\n",
    "ls_fit = LinearRegression().fit(xs, ys)\n",
    "ls_beta = np.array([ls_fit.intercept_[0], *ls_fit.coef_[0]])\n",
    "ridge_fit = Ridge(alpha=1).fit(xs, ys)\n",
    "ridge_beta = np.array([ridge_fit.intercept_[0], *ridge_fit.coef_])\n",
    "lasso_fit = Lasso(alpha=1).fit(xs, ys)\n",
    "lasso_beta = np.array([lasso_fit.intercept_[0], *lasso_fit.coef_])\n",
    "\n",
    "print(\"Least Squares:\", ls_beta)\n",
    "print(\"Ridge:\", ridge_beta)\n",
    "print(\"Lasso:\", lasso_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf36e7-e37f-4f46-ba00-9f492ef9c1ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# plot model predictions\n",
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "\n",
    "ax2 = fig.add_subplot(131)\n",
    "plot_predictions_grid(ridge_fit, \"Ridge\", fig, ax2)\n",
    "ax3 = fig.add_subplot(132)\n",
    "plot_predictions_grid(lasso_fit, \"Lasso\", fig, ax3)\n",
    "\n",
    "# plot colorbar\n",
    "ax_cbar = fig.add_subplot(133)\n",
    "ax_cbar.set_title(\"colorbar\")\n",
    "draw_colorbar(\"viridis\", fig, ax_cbar)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2630bca-5ece-4945-adad-3bc1ac1b0e39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "The next two code cells give you an interactive view of two $x$ variables, the loss and the parameter path for different regularizations.\n",
    "Feel free to play around with the sliders and see how everything reacts!\n",
    "\n",
    "For example, you can use the `x1_x2_angle` slider to control the angle between the variables in observation space.\n",
    "This controls correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df30c3-50cf-4218-9661-cdc74cf228c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. When variables are less extremely correlated, the optimum gets clearer in the loss surface. What would be a reason to still use regularization?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc1594-7a95-4868-a1cf-731dd3d78236",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf476c-11a5-4c13-9468-c201f654976e",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def orthogonal_projection(X, y):\n",
    "    try:\n",
    "        return X @ np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print(repr(e))\n",
    "        return None\n",
    "\n",
    "\n",
    "def fig_to_img(fig):\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    frame = buf.read()\n",
    "    plt.close(fig)\n",
    "    return frame\n",
    "\n",
    "\n",
    "# only the 3d vector projection plot without interactivity\n",
    "def render3d(X, y, y_proj, X_surf, Y_surf, Z_surf, elev, angle, ax):\n",
    "    if X.shape[1] == 2:\n",
    "        ax.plot_surface(X_surf, Y_surf, Z_surf, alpha=0.25, color=\"cyan\", edgecolor=\"none\", zorder=0)\n",
    "\n",
    "    lim = 1.2 * np.max(np.abs(np.vstack([*X.T, y])))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "    ax.set_zlim(-lim, lim)\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        ax.quiver(0, 0, 0, *X[:, i], color=\"b\", arrow_length_ratio=0.15, linewidth=1)\n",
    "    ax.quiver(0, 0, 0, *y, color=\"r\", arrow_length_ratio=0.15, linewidth=1)\n",
    "    ax.quiver(0, 0, 0, *y_proj, color=\"r\", arrow_length_ratio=0.15, linewidth=1, linestyle=\"dashed\")\n",
    "    ax.quiver(*y_proj, *(y-y_proj), color=\"gray\", arrow_length_ratio=0.15, linewidth=1)\n",
    "    ax.view_init(elev=elev, azim=angle)\n",
    "    ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "    ax.text(X[0, 0], X[1, 0], X[2, 0], \"x1\")\n",
    "    if X.shape[1] > 1:\n",
    "        ax.text(X[0, 1], X[1, 1], X[2, 1], \"x2\")\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    # custom coordinate lines\n",
    "    for axis in np.eye(3):\n",
    "        ax.plot([0, axis[0]], [0, axis[1]], [0, axis[2]], color=\"black\", lw=1)\n",
    "    ax.text(1.1, 0, 0, \"o1\")\n",
    "    ax.text(0, 1.1, 0, \"o2\")\n",
    "    ax.text(0, 0, 1.1, \"o3\")\n",
    "    \n",
    "    ax.set_title(\"Variables in Observation Space\")\n",
    "\n",
    "\n",
    "# only the 2d regression line plot without interactivity\n",
    "def plot_colspace_preds_1d(X, y, ax):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # set axes limits\n",
    "    xlim = (-1, 1)\n",
    "    ylim = (-1, 1)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    # show gridlines\n",
    "    ax.grid()\n",
    "\n",
    "    # add lines for axes\n",
    "    ax.axvline(0, xlim[0], xlim[1], color=\"black\", zorder=0)\n",
    "    ax.axhline(0, ylim[0], ylim[1], color=\"black\", zorder=0)\n",
    "\n",
    "    ax.scatter(X[:, 0], y)\n",
    "    \n",
    "    beta_intermediate = X.T @ y\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    print(f\"X^Ty = {beta_intermediate}\")\n",
    "    print(f\"(X^TX)^-1X^Ty = {beta}\")\n",
    "\n",
    "    xs_samples = np.zeros_like(X[:2])\n",
    "    xs_samples[0, 0] = -1\n",
    "    xs_samples[1, 0] = 1\n",
    "    ys_samples1 = colspace_to_rowspace(xs_samples, beta_intermediate)\n",
    "    ax.plot(xs_samples[:, 0], ys_samples1, label=r\"$\\hat{\\beta}=\\mathbf{X}^T\\mathbf{y}$\")\n",
    "\n",
    "    ys_samples2 = colspace_to_rowspace(xs_samples, beta)\n",
    "    ax.plot(xs_samples[:, 0], ys_samples2, label=r\"$\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    # define annotations\n",
    "    ax.set_title(\"Predictions\")\n",
    "    ax.set_xlabel(r\"$\\mathbf{x}_1$\")\n",
    "    ax.set_ylabel(r\"$\\hat{\\mathbf{y}}$\")\n",
    "\n",
    "\n",
    "def get_regularized_model(fit_intercept, regularization_lambda, regularization_alpha, regularization):\n",
    "    model = None\n",
    "    \n",
    "    if regularization == \"Lasso\":\n",
    "        model = Lasso(fit_intercept=fit_intercept, alpha=regularization_lambda)\n",
    "    elif regularization == \"Ridge\":\n",
    "        model = Ridge(fit_intercept=fit_intercept, alpha=regularization_lambda)\n",
    "    elif regularization == \"ElasticNet\":\n",
    "        model = ElasticNet(fit_intercept=fit_intercept, alpha=regularization_lambda, l1_ratio=1-regularization_alpha)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def lasso_terms(n_samples, regularization_lambda, betas):\n",
    "    return 2 * n_samples * regularization_lambda * np.sum(np.abs(betas), axis=0)\n",
    "    \n",
    "\n",
    "def ridge_terms(regularization_lambda, betas):\n",
    "    return regularization_lambda * np.sum(np.square(betas), axis=0)\n",
    "\n",
    "\n",
    "def elasticnet_terms(n_samples, regularization_lambda, regularization_alpha, betas):\n",
    "    return regularization_lambda * (n_samples * regularization_alpha * ridge_terms(1, betas) + (1 - regularization_alpha) * lasso_terms(n_samples, 1, betas))\n",
    "\n",
    "\n",
    "# TODO: clean up loss functions to match sklearn (move n_samples factor for Lasso and ElasticNet to match sklearn losses)\n",
    "# currently they have the same minimum but different scaling than what sklearn uses\n",
    "def get_regularization_terms(n_samples, regularization_lambda, regularization_alpha, betas, regularization):\n",
    "    terms = None\n",
    "\n",
    "    if regularization == \"Lasso\":\n",
    "        terms = lasso_terms(n_samples, regularization_lambda, betas)\n",
    "    elif regularization == \"Ridge\":\n",
    "        terms = ridge_terms(regularization_lambda, betas)\n",
    "    elif regularization == \"ElasticNet\":\n",
    "        terms = elasticnet_terms(n_samples, regularization_lambda, regularization_alpha, betas)\n",
    "\n",
    "    return terms\n",
    "\n",
    "\n",
    "def plot_loss_2d(X, y, fit_intercept, regularization_lambda, regularization_alpha, regularization, resolution, print_values, fig, ax):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # set axes limits\n",
    "    axlim = (-2, 2)\n",
    "    ax.set_xlim(axlim)\n",
    "    ax.set_ylim(axlim)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    \n",
    "    # add lines for axes\n",
    "    ax.axvline(0, axlim[0], axlim[1], color=\"black\", zorder=0)\n",
    "    ax.axhline(0, axlim[0], axlim[1], color=\"black\", zorder=0)\n",
    "\n",
    "    coords = np.linspace(axlim[0], axlim[1], resolution)\n",
    "    pts_x, pts_y = np.meshgrid(coords, coords)\n",
    "    betas = np.array([np.ravel(pts_x), np.ravel(pts_y)])\n",
    "\n",
    "    losses = np.sum((np.repeat(np.array([y]).T, resolution**2, axis=1) - (X @ betas))**2, axis=0)\n",
    "    losses = np.reshape(losses, (resolution, resolution))\n",
    "    im = ax.imshow(losses, extent=(axlim[0], axlim[1], axlim[1], axlim[0]))\n",
    "    fig.colorbar(im, ax=ax, label=\"RSS\")\n",
    "    ax.contour(pts_x, pts_y, losses, levels=np.linspace(losses.min(), losses.max(), 10), colors=\"white\")\n",
    "    \n",
    "    ax.axhline(0, axlim[0], axlim[1], color=\"white\")\n",
    "    ax.axvline(0, axlim[0], axlim[1], color=\"white\")\n",
    "\n",
    "    # opt_idx = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    # opt = (pts_x[opt_idx], pts_y[opt_idx])\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model = model.fit(X, y)\n",
    "    opt = model.coef_\n",
    "    if print_values:\n",
    "        print(f\"OLS optimum = [{opt[0]:.2f}, {opt[1]:.2f}]\")\n",
    "    ax.scatter(opt[0], opt[1], color=\"red\", marker=\"*\", label=\"OLS optimum\", zorder=10)\n",
    "\n",
    "    regularized_opts = np.zeros((100, 2))\n",
    "    max_lambda = 1 if regularization == \"Lasso\" else 10\n",
    "    for i, lambda_i in enumerate(np.linspace(0, max_lambda, 100)):\n",
    "        if i == 0:\n",
    "            regularized_opts[i] = model.coef_\n",
    "            continue\n",
    "        model_i = get_regularized_model(fit_intercept, lambda_i, regularization_alpha, regularization)\n",
    "        model_i = model_i.fit(X, y)\n",
    "        regularized_opts[i] = model_i.coef_\n",
    "    ax.plot(regularized_opts[:, 0], regularized_opts[:, 1])\n",
    "\n",
    "    model = get_regularized_model(fit_intercept, regularization_lambda, regularization_alpha, regularization)\n",
    "    model = model.fit(X, y)\n",
    "    regularized_opt = model.coef_\n",
    "    if print_values:\n",
    "        print(f\"regularized optimum = [{regularized_opt[0]:.2f}, {regularized_opt[1]:.2f}]\")\n",
    "    ax.scatter(regularized_opt[0], regularized_opt[1], color=\"red\", label=f\"{regularization} optimum\", zorder=10)\n",
    "\n",
    "    regularization_terms_func = lambda x: get_regularization_terms(len(X), regularization_lambda, regularization_alpha, x, regularization)\n",
    "    regularization_terms = regularization_terms_func(betas)\n",
    "    regularization_terms = np.reshape(regularization_terms, (resolution, resolution))\n",
    "    regularization_losses = losses + regularization_terms\n",
    "    \n",
    "    ax.contour(pts_x, pts_y, losses, levels=[np.sum((y - (X @ regularized_opt))**2)], colors=\"red\")\n",
    "    ax.contour(pts_x, pts_y, regularization_terms, levels=[regularization_terms_func(regularized_opt)], colors=\"red\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Loss landscape\")\n",
    "    ax.set_xlabel(r\"$\\beta_1$\")\n",
    "    ax.set_ylabel(r\"$\\beta_2$\")\n",
    "\n",
    "\n",
    "def plot_loss_on_optima_path(X, y, fit_intercept, regularization_lambda, regularization_alpha, regularization, ax):\n",
    "    lambdas_resolution = 100\n",
    "    losses = np.zeros((lambdas_resolution,))\n",
    "    max_lambda = 1 if regularization == \"Lasso\" else 10\n",
    "    lambdas = np.linspace(0, max_lambda, lambdas_resolution+1)[1:]\n",
    "\n",
    "    for i, lambda_i in enumerate(lambdas):\n",
    "        model_i = get_regularized_model(fit_intercept, lambda_i, regularization_alpha, regularization)\n",
    "        model_i = model_i.fit(X, y)\n",
    "\n",
    "        # betas = [model_i.intercept_, *model_i.coef_]\n",
    "        betas = model_i.coef_\n",
    "        losses[i] = np.sum(np.square(y - model_i.predict(X))) + get_regularization_terms(len(X), regularization_lambda, regularization_alpha, betas, regularization)\n",
    "\n",
    "    model_conf = get_regularized_model(fit_intercept, regularization_lambda, regularization_alpha, regularization)\n",
    "    model_conf = model_conf.fit(X, y)\n",
    "    \n",
    "    # betas = [model_conf.intercept_, *model_conf.coef_]\n",
    "    betas = model_conf.coef_\n",
    "    loss_conf = np.sum(np.square(y - model_conf.predict(X))) + get_regularization_terms(len(X), regularization_lambda, regularization_alpha, betas, regularization)\n",
    "    \n",
    "    ax.plot(lambdas, losses)\n",
    "    \n",
    "    ax.scatter([0], [losses[0]], marker=\"*\", color=\"red\", label=\"OLS optimum\", zorder=10)\n",
    "    ax.scatter([regularization_lambda], [loss_conf], marker=\"o\", color=\"red\", label=f\"{regularization} optimum\", zorder=10)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylim((0, np.max(losses)*1.2))\n",
    "    ax.set_ylabel(\"Regularized loss\")\n",
    "    ax.set_xlabel(r\"$\\lambda$\")\n",
    "    ax.set_title(\"Regularized loss along regularization optima path\")\n",
    "\n",
    "\n",
    "def plot_betas_per_lambda(X, y, fit_intercept, regularization_lambda, regularization_alpha, regularization, ax):\n",
    "    lambdas_resolution = 100\n",
    "    betas = np.zeros((lambdas_resolution, 3))\n",
    "    max_lambda = 1 if regularization == \"Lasso\" else 10\n",
    "    lambdas = np.linspace(0, max_lambda, lambdas_resolution+1)[1:]\n",
    "\n",
    "    for i, lambda_i in enumerate(lambdas):\n",
    "        model_i = get_regularized_model(fit_intercept, lambda_i, regularization_alpha, regularization)\n",
    "        model_i = model_i.fit(X, y)\n",
    "\n",
    "        betas[i] = [model_i.intercept_, *model_i.coef_]\n",
    "        # betas[i] = model_i.coef_\n",
    "\n",
    "    for i in range(betas.shape[1]):\n",
    "        ax.plot(lambdas, betas[:, i], label=r\"$\\beta_\"+str(i)+r\"$\")\n",
    "\n",
    "    ax.axvline(regularization_lambda, np.min(betas), np.max(betas), linestyle=\"dashed\", color=\"red\", zorder=-10)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"Learned value\")\n",
    "    ax.set_xlabel(r\"$\\lambda$\")\n",
    "    ax.set_title(\"Regularization influence on model parameters\")\n",
    "\n",
    "\n",
    "def _normalize(v):\n",
    "    n = np.linalg.norm(v)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Zero vector cannot be normalized.\")\n",
    "    return v / n\n",
    "\n",
    "def _axis_angle_rotate(v, axis, angle):\n",
    "    axis = _normalize(axis)\n",
    "    v_par = np.dot(v, axis) * axis\n",
    "    v_perp = v - v_par\n",
    "    w = np.cross(axis, v_perp)\n",
    "    return v_par + v_perp * np.cos(angle) + w * np.sin(angle)\n",
    "\n",
    "def generate_vectors(y, theta_hy, phi_h, s1, phi_out, theta_out, s2):\n",
    "    \"\"\"\n",
    "    y: base vector\n",
    "    theta_hy: angle between h and y\n",
    "    phi_h: rotation of h around y\n",
    "    s1: scale of h relative to ||y||\n",
    "    phi_out: rotation of output pair around h\n",
    "    theta_out: angle between each output vector and h\n",
    "    s2: relative scale factor of the two outputs\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    yn = _normalize(y)\n",
    "\n",
    "    # Step 1: construct h\n",
    "    # pick any vector not parallel to y\n",
    "    tmp = np.array([1.0, 0.0, 0.0])\n",
    "    if np.allclose(np.cross(yn, tmp), 0):\n",
    "        tmp = np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "    # perpendicular direction\n",
    "    b = _normalize(np.cross(yn, tmp))\n",
    "\n",
    "    # initial h direction with angle theta_hy\n",
    "    h_dir = np.cos(theta_hy) * yn + np.sin(theta_hy) * b\n",
    "\n",
    "    # spin around y by phi_h\n",
    "    h_dir = _axis_angle_rotate(h_dir, yn, phi_h)\n",
    "    h = h_dir * (s1 * np.linalg.norm(y))\n",
    "\n",
    "    # Step 2: construct basis around h\n",
    "    hn = _normalize(h)\n",
    "    # pick perpendicular vector\n",
    "    tmp2 = np.array([1.0, 0.0, 0.0])\n",
    "    if np.allclose(np.cross(hn, tmp2), 0):\n",
    "        tmp2 = np.array([0.0, 1.0, 0.0])\n",
    "    p = _normalize(np.cross(hn, tmp2))   # perpendicular to h\n",
    "    q = np.cross(hn, p)                  # completes right-handed basis\n",
    "\n",
    "    # Step 3: construct raw u and v directions at angle theta_out from h\n",
    "    # before rotating around h\n",
    "    u_dir = np.cos(theta_out) * hn + np.sin(theta_out) * p\n",
    "    v_dir = np.cos(theta_out) * hn - np.sin(theta_out) * p  # opposite side\n",
    "\n",
    "    # Step 4: spin both around h by phi_out\n",
    "    u_dir = _axis_angle_rotate(u_dir, hn, phi_out)\n",
    "    v_dir = _axis_angle_rotate(v_dir, hn, phi_out)\n",
    "\n",
    "    # Step 5: apply length scalings\n",
    "    h_norm = np.linalg.norm(h)\n",
    "    u = u_dir * ((s2 if s2 <= 1.0 else (1.0 / (2-s2))) * h_norm)\n",
    "    v = v_dir * (((1.0/s2) if s2 <= 1.0 else (2-s2)) * h_norm)\n",
    "\n",
    "    return h, u, v\n",
    "\n",
    "\n",
    "def rotate_vectors_about_plane_normal(v1, v2, angle):\n",
    "    # Normal of the plane spanned by v1 and v2\n",
    "    n = np.cross(v1, v2)\n",
    "    n_norm = np.linalg.norm(n)\n",
    "    if n_norm == 0:\n",
    "        raise ValueError(\"Input vectors are collinear; plane normal undefined.\")\n",
    "    k = n / n_norm  # unit normal\n",
    "\n",
    "    # Rodrigues' rotation matrix\n",
    "    K = np.array([[0, -k[2], k[1]],\n",
    "                  [k[2], 0, -k[0]],\n",
    "                  [-k[1], k[0], 0]])\n",
    "    I = np.eye(3)\n",
    "    R = I + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n",
    "\n",
    "    return R @ v1, R @ v2\n",
    "\n",
    "\n",
    "# interactive 3d vector projection, loss surface and parameter trajectories\n",
    "def interactive_vector_rotation(y, x_labels=None, n_frames=90, elev=20, proj_func=orthogonal_projection):\n",
    "    if x_labels is None:\n",
    "        x_labels = [\"\"]*2\n",
    "    \n",
    "    def plot(viewangle=315., x1_x2_yangle=180., x1_x2_angle=45., x1_x2_relative_scale=1., centering=False, unit_variance=False, fit_intercept=False, lambda_regularization=0.5, alpha_regularization=0.5, regularization=\"Lasso\", print_values=False):\n",
    "        theta_hy = np.deg2rad(30.)\n",
    "        phi_h = np.deg2rad(135.)\n",
    "        s1=1\n",
    "        phi_out = np.deg2rad(165.)\n",
    "        x1_x2_angle = np.deg2rad(x1_x2_angle-180.)\n",
    "        h, u, v = generate_vectors(y, theta_hy, phi_h, s1, phi_out, x1_x2_angle, x1_x2_relative_scale)\n",
    "        ur, vr = rotate_vectors_about_plane_normal(u, v, np.deg2rad(x1_x2_yangle))\n",
    "        X = np.stack([ur, vr], axis=1)\n",
    "        resolution = 100\n",
    "        \n",
    "        if regularization != \"Lasso\":\n",
    "            lambda_regularization *= 10\n",
    "\n",
    "        if print_values:\n",
    "            print(f\"X = \\n{X}\")\n",
    "            print(f\"y = {y}\")\n",
    "\n",
    "        scaler = StandardScaler(with_mean=centering, with_std=unit_variance)\n",
    "        scaler.fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X = X_scaled\n",
    "\n",
    "        if print_values:\n",
    "            print(f\"Preprocessed X = \\n{X}\")\n",
    "            print(f\"beta = {np.linalg.inv(X.T @ X) @ X.T @ y}\")\n",
    "\n",
    "        ####################################\n",
    "        ############## PLANE ###############\n",
    "        X_surf = None\n",
    "        Y_surf = None\n",
    "        Z_surf = None\n",
    "        if X.shape[1] == 2:\n",
    "            # plane grid in parametric form: p(s,t) = s*x1 + t*x2\n",
    "            span = 1\n",
    "            s = np.linspace(-span, span, 10)\n",
    "            t = np.linspace(-span, span, 10)\n",
    "            S, T = np.meshgrid(s, t)\n",
    "            P = np.outer(S, X[:, 0]) + np.outer(T, X[:, 1])\n",
    "            X_surf, Y_surf, Z_surf = P[:, 0].reshape(S.shape), P[:, 1].reshape(S.shape), P[:, 2].reshape(S.shape)\n",
    "        ####################################\n",
    "        \n",
    "        y_proj = proj_func(X, y)\n",
    "        if print_values:\n",
    "            print(f\"y_proj = {y_proj}\")\n",
    "\n",
    "        fig1 = plt.figure(figsize=(25, 6))\n",
    "        ax1 = fig1.add_subplot(141, projection=\"3d\")\n",
    "        render3d(X, y, y_proj, X_surf, Y_surf, Z_surf, elev, viewangle, ax1)\n",
    "\n",
    "        ax2 = fig1.add_subplot(142)\n",
    "        # plot_colspace_preds_1d(X, y, ax2)\n",
    "        plot_loss_2d(X, y, fit_intercept, lambda_regularization, alpha_regularization, regularization, resolution, print_values, fig1, ax2)\n",
    "\n",
    "        ax3 = fig1.add_subplot(143)\n",
    "        plot_loss_on_optima_path(X, y, fit_intercept, lambda_regularization, alpha_regularization, regularization, ax3)\n",
    "        \n",
    "        ax4 = fig1.add_subplot(144)\n",
    "        plot_betas_per_lambda(X, y, fit_intercept, lambda_regularization, alpha_regularization, regularization, ax4)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    interactive_plot = interactive(plot,\n",
    "                                   viewangle=(0., 359., 5.),\n",
    "                                   x1_x2_yangle=(0., 365., 1.),\n",
    "                                   x1_x2_angle=(1., 89., 1.),\n",
    "                                   x1_x2_relative_scale=(0.01, 1.99, 0.01),\n",
    "                                   centering=False,\n",
    "                                   unit_variance=False,\n",
    "                                   fit_intercept=False,\n",
    "                                   lambda_regularization=(0.01, 1., 0.01),\n",
    "                                   alpha_regularization=(0.01, 0.99, 0.01),\n",
    "                                   regularization=[\"Lasso\", \"Ridge\", \"ElasticNet\"],\n",
    "                                   print_values=False)\n",
    "    output = interactive_plot.children[-1]\n",
    "    output.layout.height = '600px'\n",
    "    return interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175367b3-2eb1-4032-a06a-0bf9a33091f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "ys_interactive = np.array([0.4, 0.4, 1.6])\n",
    "interactive_vector_rotation(ys_interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199e410",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "## Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27ceaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Skip the following two code cells.\n",
    "They just prepare the plots for later on and generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c456ab7",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_ground_truth(X, Y):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 1)\n",
    "    ax2.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], s=10, c=\"Red\")\n",
    "    ax2.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], s=10, c=\"Blue\")\n",
    "    # plt.plot([-1, 2], [-1, 2], c=\"Gray\")\n",
    "    ax2.set_xlabel(\"x1\")\n",
    "    ax2.set_ylabel(\"x2\")\n",
    "    ax2.set_xlim(-0.25, 1.25)\n",
    "    ax2.set_ylim(-0.25, 1.25)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(\"Noisy Points with Classes - 2D View\")\n",
    "    ax2.legend([\"Noisy Points of Class 1\", \"Noisy Points of Class 0\"])\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 2, projection=\"3d\", computed_zorder=False)\n",
    "    ax1.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], 1, s=10, c=\"Red\", zorder=1)\n",
    "    ax1.scatter(X[Y == 0][:, 0], X[Y == 0][:, 1], 0, s=10, c=\"Blue\", zorder=-1)\n",
    "    ax1.set_xlabel(\"x1\")\n",
    "    ax1.set_ylabel(\"x2\")\n",
    "    ax1.set_zlabel(\"y\")\n",
    "    ax1.set_xlim(-0.25, 1.25)\n",
    "    ax1.set_ylim(-0.25, 1.25)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(\"Noisy Points with Classes - 3D View\")\n",
    "    ax1.legend([\"Noisy Points of Class 1\", \"Noisy Points of Class 0\"])\n",
    "    ax1.view_init(elev=20, azim=-115)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_linear_model_preds(X, Y_pred_in, xlim_left=-0.25, xlim_right=1.25, ylim_bottom=-0.25, ylim_top=1.25):\n",
    "    plane_x1 = [xlim_left, xlim_right]\n",
    "    plane_x2 = [ylim_bottom, ylim_top]\n",
    "    plane_x1, plane_x2 = np.meshgrid(plane_x1, plane_x2)\n",
    "    plane_y = 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 1)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], s=10, c=Y_pred_in)\n",
    "    ax2.set_xlabel(\"x1\")\n",
    "    ax2.set_ylabel(\"x2\")\n",
    "    ax2.set_xlim(xlim_left, xlim_right)\n",
    "    ax2.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(\"Points with Regression Prediction - 2D View\")\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 2, projection=\"3d\", computed_zorder=False)\n",
    "    ax1.scatter(X[Y_pred_in > plane_y][:, 0], X[Y_pred_in > plane_y][:, 1], Y_pred_in[Y_pred_in > plane_y], s=10, c=Y_pred_in[Y_pred_in > plane_y], zorder=1, vmin=np.min(Y_pred_in))\n",
    "    ax1.scatter(X[Y_pred_in <= plane_y][:, 0], X[Y_pred_in <= plane_y][:, 1], Y_pred_in[Y_pred_in <= plane_y], s=10, c=Y_pred_in[Y_pred_in <= plane_y], zorder=-1, vmax=np.max(Y_pred_in))\n",
    "    ax1.plot_surface(plane_x1, plane_x2, np.zeros_like(plane_x1) + plane_y, color=[0.5, 0.5, 0.5], alpha=0.75, zorder=0)\n",
    "    ax1.set_xlabel(\"x1\")\n",
    "    ax1.set_ylabel(\"x2\")\n",
    "    ax1.set_zlabel(\"y-hat\")\n",
    "    ax1.set_xlim(xlim_left, xlim_right)\n",
    "    ax1.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(\"Points with Regression Prediction - 3D View\")\n",
    "    ax1.view_init(elev=20, azim=-115)\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    ax.scatter(X[Y_pred_in >= plane_y][:, 0], X[Y_pred_in >= plane_y][:, 1], s=10, c=\"Red\")\n",
    "    ax.scatter(X[Y_pred_in < plane_y][:, 0], X[Y_pred_in < plane_y][:, 1], s=10, c=\"Blue\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_xlim(xlim_left, xlim_right)\n",
    "    ax.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(\"Points with Classification Prediction - 2D View\")\n",
    "    ax.legend([\"Points Predicted in Class 1\", \"Points Predicted in Class 0\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_logistic_model_preds(X, Y_pred_in, xlim_left=-0.25, xlim_right=1.25, ylim_bottom=-0.25, ylim_top=1.25):\n",
    "    plane_x1 = [xlim_left, xlim_right]\n",
    "    plane_x2 = [ylim_bottom, ylim_top]\n",
    "    plane_x1, plane_x2 = np.meshgrid(plane_x1, plane_x2)\n",
    "    plane_y = 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 1)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], s=10, c=Y_pred_in)\n",
    "    ax2.set_xlabel(\"x1\")\n",
    "    ax2.set_ylabel(\"x2\")\n",
    "    ax2.set_xlim(xlim_left, xlim_right)\n",
    "    ax2.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(\"Points with Regression Prediction - 2D View\")\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 2, projection=\"3d\", computed_zorder=False)\n",
    "    ax1.scatter(X[Y_pred_in > plane_y][:, 0], X[Y_pred_in > plane_y][:, 1], Y_pred_in[Y_pred_in > plane_y], s=10, c=Y_pred_in[Y_pred_in > plane_y], zorder=1, vmin=np.min(Y_pred_in))\n",
    "    ax1.scatter(X[Y_pred_in <= plane_y][:, 0], X[Y_pred_in <= plane_y][:, 1], Y_pred_in[Y_pred_in <= plane_y], s=10, c=Y_pred_in[Y_pred_in <= plane_y], zorder=-1, vmax=np.max(Y_pred_in))\n",
    "    ax1.plot_surface(plane_x1, plane_x2, np.zeros_like(plane_x1) + plane_y, color=[0.5, 0.5, 0.5], alpha=0.75, zorder=0)\n",
    "    ax1.set_xlabel(\"x1\")\n",
    "    ax1.set_ylabel(\"x2\")\n",
    "    ax1.set_zlabel(\"y-hat\")\n",
    "    ax1.set_xlim(xlim_left, xlim_right)\n",
    "    ax1.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(\"Points with Regression Prediction - 3D View\")\n",
    "    ax1.view_init(elev=20, azim=-115)\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    ax.scatter(X[Y_pred_in >= plane_y][:, 0], X[Y_pred_in >= plane_y][:, 1], s=10, c=\"Red\")\n",
    "    ax.scatter(X[Y_pred_in < plane_y][:, 0], X[Y_pred_in < plane_y][:, 1], s=10, c=\"Blue\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_xlim(xlim_left, xlim_right)\n",
    "    ax.set_ylim(ylim_bottom, ylim_top)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(\"Points with Regression Prediction - 2D View\")\n",
    "    ax.legend([\"Points Predicted in Class 1\", \"Points Predicted in Class 0\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def are_linear_predictions_equal(X, model, custom_func):\n",
    "    model_Y_pred = model.predict(X)\n",
    "    custom_Y_pred = np.apply_along_axis(custom_func, 1, X)\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    if np.all(np.abs(custom_Y_pred - model_Y_pred) < epsilon):\n",
    "        print(\"You got it right!\")\n",
    "    else:\n",
    "        print(\"Not quite there yet. Try again!\")\n",
    "\n",
    "\n",
    "def are_logistic_predictions_equal(X, model, custom_func):\n",
    "    model_Y_pred = model.predict_proba(X)[:, 1]\n",
    "    custom_Y_pred = np.apply_along_axis(custom_func, 1, X)\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    if np.all(np.abs(custom_Y_pred - model_Y_pred) < epsilon):\n",
    "        print(\"You got it right!\")\n",
    "    else:\n",
    "        print(\"Not quite there yet. Try again!\")\n",
    "\n",
    "\n",
    "def model_to_str(m):\n",
    "    return f\"y_pred = {m.intercept_:.4f}\" + \"\".join([f\" + {coef:.4f}*x_{i}\" for i, coef in enumerate(m.coef_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59e1ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "beta = [0, -1, 1]\n",
    "X_raw = np.random.uniform(0, 1, (1000, 2))\n",
    "X = X_raw + np.random.normal(0, 0.1, (1000, 2))\n",
    "\n",
    "Y_soft = np.array([np.dot(beta, [1, x1, x2]) for x1, x2 in X_raw])\n",
    "Y = np.array([0 if y_soft < 0 else 1 for y_soft in Y_soft])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e65c1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Let's look at a dataset with two independent variables $x_{i1}$ and $x_{i2}$.\n",
    "The dependent variable $y_i$ takes on either the value $0$ or $1$ (i.e. $y_i \\in \\{0,1\\}$).\n",
    "\n",
    "The plot on the left side highlights the categorical nature of this data, ready to be classified.\n",
    "The 3D plot shows that the points can also be represented three-dimensionally with the values of $y_i$ as the third coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dad458-0cbc-4b31-8ce4-7b08e78bdd57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Even though it is a representation of the same (categorical) attribute $y$, can you think of a regression function for $y$ in the three-dimensional space? What would it look like geometrically?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d2beb-daa1-4e55-8abc-2c1145106930",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6966762",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_ground_truth(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a879cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Let's try this idea using `sklearn.linear_model.LinearRegression`.\n",
    "This model is fitted according to the ordinary least squares solution of the linear regression model on the values of $x_{i1}$ and $x_{i2}$.\n",
    "\n",
    "You can see the prediction results of the resulting model on the training data in the first plot of the code cell after the next.\n",
    "The middle plot shows the points in 3D space where the height of the points is taken from the regression prediction $\\hat{y}$.\n",
    "You should be able to see that these points lie on the regression plane that you have described in the task above.\n",
    "\n",
    "Side note:\n",
    "\n",
    "This is similar to the actual implementation of `sklearn.linear_model.RidgeClassifier`, about which its documentation says:\n",
    "> This classifier first converts the target values into {-1, 1} and then treats the problem as a regression task (multi-output regression in the multiclass case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45140f-c611-41d1-92e2-e93933ab875a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. How can the gray plane in the 3D plot be used to get the classification result in the plot on the right?\n",
    "2. The answer to the last question implies a decision boundary in the plot on the right. How does that look like?\n",
    "3. BONUS: Can you describe that decision boundary from the regression plane and the gray plane in the 3D plot geometrically?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3972e76-dcc0-43f9-ab52-16b89f863f73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1650af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "zero_one_loss(Y, Y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699e55a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_linear_model_preds(X, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d7baa-c537-422c-ad7c-da2b1c0dfae2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Implement the linear regression model in the function `linear_regressor()`. The input variable `x` and the variable `beta_vec` contain all the values you need for the computation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd267f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def linear_regressor(x):\n",
    "    beta0 = model.intercept_\n",
    "    beta1_n = model.coef_\n",
    "    beta_vec = np.array([beta0, *beta1_n])\n",
    "\n",
    "    # TODO: implement the linear regression model function for the parameters beta\n",
    "    return None\n",
    "\n",
    "\n",
    "# checking and plotting the solution\n",
    "custom_Y_pred = np.apply_along_axis(linear_regressor, 1, X)\n",
    "\n",
    "if not all(custom_Y_pred == None):\n",
    "    plot_linear_model_preds(X, custom_Y_pred)\n",
    "    are_linear_predictions_equal(X, model, linear_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133870e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "... and that is how you can use a linear regression model for classification!\n",
    "\n",
    "In the lecture you have heard about a problem with this approach when there is an outlier to the training data.\n",
    "Have a look at that case in the next three code blocks.\n",
    "\n",
    "This is what happens here on a high level:\n",
    "1. Adding a single outlier point to the data.\n",
    "2. Fitting a new linear regression model to that data.\n",
    "3. Plotting the prediction results of that new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b645f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "X_outlier = np.concat([X, [[10, 0]]])\n",
    "Y_outlier = np.concat([Y, [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a0dfa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_outlier, Y_outlier)\n",
    "\n",
    "Y_pred = model.predict(X_outlier)\n",
    "\n",
    "zero_one_loss(Y_outlier, Y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da18ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_linear_model_preds(X_outlier, Y_pred, xlim_left=-1, xlim_right=11, ylim_top=2, ylim_bottom=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc1886",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "We can clearly see that the decision boundary is tilted towards the right now in the plot on the right and that the $0$-$1$ loss is larger now than before.\n",
    "\n",
    "In the lecture it was claimed that an outlier does not have such a large effect on a logistic regression model.\n",
    "See for yourself in the following two code blocks, where the model is trained and the results are plotted again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0d165",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X_outlier, Y_outlier)\n",
    "\n",
    "Y_pred = model.predict_proba(X_outlier)[:, 1]\n",
    "\n",
    "zero_one_loss(Y_outlier, Y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e070e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_logistic_model_preds(X_outlier, Y_pred, xlim_left=-1, xlim_right=11, ylim_top=2, ylim_bottom=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9fd53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "This is caused by the different loss that is used for the optimization in logistic regression.\n",
    "You are encouraged to think about how the different loss has a different influence on the fitting process.\n",
    "The basic idea is that here, wrongly predicted outliers are not penalized as much as with the sum of squared errors, which is used in the usual linear regression setting.\n",
    "(Take it as a bonus task to compare the influence of an outlier on the logistic regression loss vs. the linear regression loss.)\n",
    "\n",
    "Let's get an intuitive understanding of how this works by looking at the plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da7f59-5847-46be-8f49-f16eecc09521",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. In each of the 3D plots, where would the outlier be positioned with the $y_i$ value from ground truth instead of from the model prediction?\n",
    "2. Compare the ground truth-based position of the outlier to the predicted positions. Which case (linear or logistic) indicates a higher deviation of the model from ground truth? How does that influence the optimization on an intuitive level?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed439699-0d49-4f3e-8fa1-e6fc24ddb2bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553774d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Enough with the comparison between linear and logistic regression for now.\n",
    "Let's have a look at just the logistic regression model!\n",
    "\n",
    "For that, a logistic regression model is trained and visualized on the data without the outlier in the following two code cells. (Just like the linear regression model in the beginning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5aeb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X, Y)\n",
    "\n",
    "Y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "zero_one_loss(Y, Y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5715e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_logistic_model_preds(X, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a2c3b-eb01-4468-805b-c6308c05db22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Implement the logistic regression model in the function `logistic_regressor()`. The input variable `x` and the variable `beta_vec` contain all the values you need for the computation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c88407",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def logistic_regressor(x):\n",
    "    beta0 = model.intercept_\n",
    "    beta1_n = model.coef_\n",
    "    beta_vec = np.array([beta0[0], *beta1_n[0]])\n",
    "\n",
    "    # TODO: implement the logistic regression model function for the parameters beta\n",
    "    return None\n",
    "\n",
    "\n",
    "# checking and plotting the solution\n",
    "custom_Y_pred = np.apply_along_axis(logistic_regressor, 1, X)\n",
    "\n",
    "if not all(custom_Y_pred == None):\n",
    "    plot_logistic_model_preds(X, custom_Y_pred)\n",
    "    are_logistic_predictions_equal(X, model, logistic_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2d7f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "In the lecture it was also mentioned that logistic regression models can struggle with data that is actually linearly separable.\n",
    "\n",
    "Side note:\n",
    "\n",
    "The logistic regression optimizer of sklearn really had to be forced into showing this behaviour, because they implemented it pretty robustly against this case.\n",
    "Check out the parameters to `LogisticRegression()` below to see what had to be done in order to disable `sklearn`'s safeguards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986b299-7d7f-4c10-9f6c-59a9b9189a55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Fill in the variable values `X_sep` and `Y_sep` with linearly separable data to see what happens to the model parameters below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368e0c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "X_sep = []\n",
    "Y_sep = []\n",
    "\n",
    "if not (len(X_sep) == 0 and len(Y_sep) == 0):\n",
    "    model = LogisticRegression(penalty=None, max_iter=1000, tol=0)\n",
    "    model.fit(X_sep, Y_sep)\n",
    "\n",
    "    print(model.coef_)\n",
    "    print(model.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
