{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e09e79-2b42-48dc-a757-1db71d21d71b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "authors:\n",
    "  - name: Tom Siegl\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25577885-c55e-498b-9e50-843fbfe26bf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "# 07: Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a09080-e544-463e-b749-356e30a94591",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "## Towards MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b107728-f485-42a9-ab06-a98ed50b76d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Today we will start by trying to solve the XOR classification problem using the linear modeling tools that we have learned about in previous sessions.\n",
    "We will see what is and isn't possible with linear methods and that the multilayer perceptron (MLP) is a natural abstraction of combining linear models.\n",
    "\n",
    "But first, have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ea3c9-f245-4cba-badd-a004c95d4b89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "std = 0.01\n",
    "X = np.concatenate([\n",
    "    np.random.multivariate_normal([1, 1], np.eye(2)*std, 100),\n",
    "    np.random.multivariate_normal([1, 0], np.eye(2)*std, 100),\n",
    "    np.random.multivariate_normal([0, 0], np.eye(2)*std, 100),\n",
    "    np.random.multivariate_normal([0, 1], np.eye(2)*std, 100),\n",
    "])\n",
    "y = np.concatenate([\n",
    "    np.full(100, 1),\n",
    "    np.full(100, 0),\n",
    "    np.full(100, 1),\n",
    "    np.full(100, 0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f62203-897f-4f08-af6a-7a66cf9d79c8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_XOR_data(xs, ys, ax):\n",
    "    ax.scatter(xs[y.astype(bool), 0], xs[y.astype(bool), 1], color=\"blue\", label=\"Class 1\")\n",
    "    ax.scatter(xs[np.invert(y.astype(bool)), 0], xs[np.invert(y.astype(bool)), 1], color=\"red\",  label=\"Class 0\")\n",
    "    ax.set_xlabel(r\"$x_1$\")\n",
    "    ax.set_ylabel(r\"$x_2$\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def get_grid_points(grid_min, grid_max, grid_size):\n",
    "    grid_vals = np.linspace(grid_min, grid_max, grid_size)\n",
    "    xx, yy = np.meshgrid(grid_vals, grid_vals)\n",
    "    return np.column_stack([xx.ravel(), yy.ravel()]), xx, yy\n",
    "\n",
    "\n",
    "def plot_predictions_grid_regression(fig, ax, xx, yy, grid_min, grid_max, preds, levels=[0.5]):\n",
    "    preds = np.reshape(preds, (len(xx), len(yy)))\n",
    "\n",
    "    im = ax.imshow(preds[::-1, :], extent=(grid_min, grid_max, grid_min, grid_max))\n",
    "    contour = ax.contour(xx, yy, preds, levels=levels, colors=\"black\")\n",
    "    ax.clabel(contour)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"predictions\")\n",
    "\n",
    "\n",
    "def plot_predictions_grid_classification(fig, ax, xx, yy, preds):\n",
    "    preds = np.reshape(preds, (len(xx), len(yy)))\n",
    "    \n",
    "    cf = ax.contourf(xx, yy, preds, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    \n",
    "    cbar = fig.colorbar(cf, ax=ax)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_label(\"predictions\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_XOR_data(X, y, ax)\n",
    "ax.set_title(\"XOR data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f3c3f-3d93-4eb6-8403-445152662ab7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "We have seen the XOR problem in the lecture, where the famous result was stated, that this simple problem cannot be solved by a linear model alone.\n",
    "\n",
    "Instead of using just one linear model, let's try to solve it by chaining linear models.\n",
    "For starters, we have the following two linear models, that each correctly classify one blob of class 1 and class 0.\n",
    "\n",
    "$$\\hat{y}_1 = 1 + (-1) \\cdot x_1 + (-1) \\cdot x_2 = 1 - x_1 - x_2$$\n",
    "\n",
    "$$\\hat{y}_2 = -1 + 1 \\cdot x_1 + 1 \\cdot x_2 = -1 + x_1 + x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be9c49-f0cd-4983-a7b1-a0f029e41734",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def get_linear_models_class_1():\n",
    "    lr1 = LinearRegression()\n",
    "    lr1.intercept_ = 1\n",
    "    lr1.coef_ = np.array([-1, -1])\n",
    "    \n",
    "    lr2 = LinearRegression()\n",
    "    lr2.intercept_ = -1\n",
    "    lr2.coef_ = np.array([1, 1])\n",
    "\n",
    "    return lr1, lr2\n",
    "\n",
    "\n",
    "def get_linear_models_binary():\n",
    "    lr1 = LinearRegression()\n",
    "    lr1.intercept_ = 0\n",
    "    lr1.coef_ = np.array([0, 1])\n",
    "    \n",
    "    lr2 = LinearRegression()\n",
    "    lr2.intercept_ = 0\n",
    "    lr2.coef_ = np.array([1, 0])\n",
    "\n",
    "    return lr1, lr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8663c-0e27-4528-85e0-c5ea87887f06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "lr1, lr2 = get_linear_models_class_1()\n",
    "# lr1, lr2 = get_linear_models_binary()  # in case you want to try something else\n",
    "\n",
    "# print model parameters\n",
    "print(\"Parameters of the first model:\", f\"beta^T = [{lr1.intercept_:.3f}, {lr1.coef_[0]:.3f}, {lr1.coef_[1]:.3f}]\")\n",
    "print(\"Parameters of the second model:\", f\"beta^T = [{lr2.intercept_:.3f}, {lr2.coef_[0]:.3f}, {lr2.coef_[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39c0bc-efcd-4a8d-a139-475e729752ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# get plot predictions\n",
    "plot_min = -0.5\n",
    "plot_max = 1.5\n",
    "grid_points, xx, yy = get_grid_points(plot_min, plot_max, 100)\n",
    "\n",
    "lr1_preds_grid = lr1.predict(grid_points)\n",
    "lr2_preds_grid = lr2.predict(grid_points)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax1, xx, yy, plot_min, plot_max, lr1_preds_grid)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Linear model 1\")\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax2, xx, yy, plot_min, plot_max, lr2_preds_grid)\n",
    "plot_XOR_data(X, y, ax2)\n",
    "ax2.set_title(\"Linear model 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160729ea-3838-405b-80ae-661c7731791d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "These two linear models both solve a subproblem of the overall problem, and it would be awesome if we could just combine them to form the overall solution.\n",
    "\n",
    "The idea is simple: When predictions from model 1 are large, then it's probably class 1 and when predictions from model 2 are large, then it's probably class 2.\n",
    "So let's try to train another linear model on the outputs of model 1 and 2 to solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da38f8f-43c6-4d64-8fb7-3fca23f25e6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "lr1_preds = lr1.predict(X)\n",
    "lr2_preds = lr2.predict(X)\n",
    "X_preds = np.stack([lr1_preds, lr2_preds], axis=-1)  # save predictions from models 1 and 2 for use as input in model 3\n",
    "\n",
    "lr3 = LinearRegression().fit(X_preds, y)  # fit model 3 on predictions from models 1 and 2\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Parameters of the second layer:\", f\"beta^T = [{lr3.intercept_:.3f}, {lr3.coef_[0]:.3f}, {lr3.coef_[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a09d83-f10e-43ca-bf2f-f374d13ce401",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "lr3_preds_grid = lr3.predict(np.stack([lr1_preds_grid, lr2_preds_grid], axis=-1))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax1, xx, yy, plot_min, plot_max, lr3_preds_grid)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Combined linear model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204c8cc-7b03-4524-a85f-a864817101c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Hm, this doesn't look right at all.\n",
    "\n",
    "To find out why this fails, we will have to look at some equations.\n",
    "Firstly we formalize our setup as:\n",
    "\n",
    "$$\\mathbf{a}^{(0)} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\qquad b^{(1)}_i = \\hat{\\beta}_{0, i}, \\qquad W^{(1)}_i = \\begin{bmatrix} \\hat{\\beta}_{1, i} & \\hat{\\beta}_{2, i} \\end{bmatrix}, \\qquad i \\in \\{1, 2\\}$$\n",
    "\n",
    "$$a^{(1)}_1 = W^{(1)}_1 \\mathbf{a}^{(0)} + b^{(1)}_1, \\qquad a^{(1)}_2 = W^{(1)}_2 \\mathbf{a}^{(0)} + b^{(1)}_2$$\n",
    "\n",
    "$$W^{(1)} = \\begin{bmatrix} W^{(1)}_1 \\\\ W^{(1)}_2 \\end{bmatrix}, \\qquad \\mathbf{b}^{(1)} = \\begin{bmatrix} b^{(1)}_1 \\\\ b^{(1)}_2 \\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{a}^{(1)} = W^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)} = \\begin{bmatrix} a^{(1)}_1 \\\\ a^{(1)}_2 \\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{a}^{(2)} &= W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190d1f0-df45-4302-b66f-4ce708266071",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. Name the variables in the equations for the outputs of linear model 1, linear model 2 and the combined linear model.\n",
    "2. Substitute and rewrite the last equation to obtain $\\mathbf{a}^{(2)} = W' \\mathbf{a}^{(0)} + b'$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb22358-198a-42e6-ad44-ef475b277dee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    "2. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871315e-649b-494e-abba-c70153493b1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "As we can see, stacking linear models doesn't really work out as we wished.\n",
    "The result is again just a linear function!\n",
    "(But you already knew that from the lecture.)\n",
    "\n",
    "Let's go back to our two linear models from the start.\n",
    "They are supposed to classify the points according to the XOR function, so maybe we should treat the classification step as a part of them.\n",
    "\n",
    "The following plots reflect this point of view, by showing the final predicted classes from each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ade4b1-b85a-4604-9d49-5c9fdd4d1a66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def threshold(x,  t=0.5):\n",
    "    return x > t\n",
    "\n",
    "lr1_preds_grid_classified = threshold(lr1.predict(grid_points))\n",
    "lr2_preds_grid_classified = threshold(lr2.predict(grid_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6e3d0-491d-4acf-a484-6dbd84d25f0d",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plot_predictions_grid_classification(fig, ax1, xx, yy, lr1_preds_grid_classified)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Linear Classifier 1\")\n",
    "\n",
    "plot_predictions_grid_classification(fig, ax2, xx, yy, lr2_preds_grid_classified)\n",
    "plot_XOR_data(X, y, ax2)\n",
    "ax2.set_title(\"Linear Classifier 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00ed45-6518-4238-9c26-34c0f881b251",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "These class outputs are not the result of a purely linear function anymore, as they include the thresholding step.\n",
    "So the argument from before, that chaining another model after these gives us still just a linear function overall, does not hold anymore.\n",
    "\n",
    "Let's try again to combine these predictions using the same kind of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcab3a1-c975-4bbb-85f1-35338b102a19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "lr1_preds_classified = threshold(lr1.predict(X))\n",
    "lr2_preds_classified = threshold(lr2.predict(X))\n",
    "X_preds_classified = np.stack([lr1_preds_classified, lr2_preds_classified], axis=-1)\n",
    "\n",
    "lr3_classified = LinearRegression().fit(X_preds_classified, y)\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Parameters of the second layer:\", f\"beta^T = [{lr3_classified.intercept_:.3f}, {lr3_classified.coef_[0]:.3f}, {lr3_classified.coef_[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1c252-b5f8-4c51-bceb-e3c15fffbe68",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "lr3_preds_grid_classified = threshold(lr3_classified.predict(np.stack([lr1_preds_grid_classified, lr2_preds_grid_classified], axis=-1)))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "plot_predictions_grid_classification(fig, ax1, xx, yy, lr3_preds_grid_classified)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Combined linear classifier\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a170889-ce01-4b7e-9dc0-e9ee791c84d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "It worked!\n",
    "\n",
    "Just one problem: We didn't train the whole thing.\n",
    "During training, we would have to somehow make sure, that the first two linear models provide sensible features for the third model to use.\n",
    "\n",
    "We know from the lecture, that MLPs are more complex than linear models and that they can be trained as a whole.\n",
    "So let's have a look at how to interpret our current setup as an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37469d91-41eb-4512-879b-1b3dffd0ac1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. How many hidden layers does our combined linear classifier have?\n",
    "2. How many neurons does the first hidden layer have in our combined linear classifier?\n",
    "3. What is the activation function in our combined linear classifier?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0af4a-ffa2-492c-a74b-c274642e038a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    "2. \n",
    "3. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97993df3-4a91-41a5-9112-37ec2c910f84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "We can see that our combined linear classifier model actually matches the structure of an MLP, so we have just constructed our first MLP model.\n",
    "\n",
    "The lecture said that to train MLPs, usually gradient descent is used.\n",
    "Let's have a look at what that means and how this affects our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196ba69-4920-4bd4-8155-f6b905f4c33e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. What is the meaning of the direction and magnitude of the gradient $\\nabla_{\\theta} L$? (**Hint**: First think about what $L$ and $\\theta$ are.)\n",
    "2. How is the gradient used to improve the model in gradient descent?\n",
    "3. Why is sigmoid a better choice than the threshold function as an activation function, when we want to train with gradient descent? (**Hint**: Look at the plots from the next code cell.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ea569-69da-447f-bb4c-02c6a3d504a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "1. \n",
    "2. \n",
    "3. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0859a26-606c-4f1b-b91a-eedd4538f148",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_activation(f, f_label, title, start, end, samples, ax):\n",
    "    samples_1d = np.linspace(start, end, samples)\n",
    "    \n",
    "    ax.plot(samples_1d, f(samples_1d), label=f_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(r\"$W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$\")\n",
    "    ax.set_ylabel(r\"$a^{(l)}$\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plot_activation(threshold, r\"$W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} > 0.5$\", \"Threshold function as activation function\", 0, 1, 1000, ax1)\n",
    "plot_activation(sigmoid, r\"$\\text{sigmoid}(W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)})$\", \"Sigmoid function as activation function\", -10, 10, 1000, ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd6517-8216-4768-921a-af6bfd55bf2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "The next cells use logistic regression models to implement the idea of replacing the threshold function with sigmoid in our combined model / MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75662d-e0ef-44d5-9af7-90bbd2a4836c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_logistic_models_class_1(xs, ys):\n",
    "    logr1 = LogisticRegression().fit(xs, ys)\n",
    "    logr1.intercept_ = 0.5\n",
    "    logr1.coef_ = np.array([-1, -1])\n",
    "    \n",
    "    logr2 = LogisticRegression().fit(xs, ys)\n",
    "    logr2.intercept_ = -1.5\n",
    "    logr2.coef_ = np.array([1, 1])\n",
    "\n",
    "    return logr1, logr2\n",
    "\n",
    "\n",
    "def get_logistic_models_binary(xs, ys):\n",
    "    logr1 = LogisticRegression().fit(xs, ys)\n",
    "    logr1.intercept_ = -0.5\n",
    "    logr1.coef_ = np.array([0, 1])\n",
    "    \n",
    "    logr2 = LogisticRegression().fit(xs, ys)\n",
    "    logr2.intercept_ = -0.5\n",
    "    logr2.coef_ = np.array([1, 0])\n",
    "\n",
    "    return logr1, logr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee6ceb-2621-49ab-b40a-ae5cac76401a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "logr1, logr2 = get_logistic_models_class_1(X, y)  # creates logistic regression models with the same parameters as linear models 1 and 2 before\n",
    "#logr1, logr2 = get_logistic_models_binary(X, y)\n",
    "\n",
    "logr1_preds_grid = logr1.predict_proba(grid_points)[:, 1]  # need to call predict_proba on sklearn's logistic regression models to get raw predictions (not yet thresholded)\n",
    "logr2_preds_grid = logr2.predict_proba(grid_points)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5244c-972c-4c14-b37f-75abee661d58",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax1, xx, yy, plot_min, plot_max, logr1_preds_grid)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Logistic regression model 1\")\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax2, xx, yy, plot_min, plot_max, logr2_preds_grid)\n",
    "plot_XOR_data(X, y, ax2)\n",
    "ax2.set_title(\"Logistic regression model 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2dbecc-8623-4651-8063-3b55f85937d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "logr1_preds = logr1.predict_proba(X)[:, 1]\n",
    "logr2_preds = logr2.predict_proba(X)[:, 1]\n",
    "\n",
    "X_preds_log = np.stack([logr1_preds, logr2_preds], axis=-1)\n",
    "\n",
    "logr3 = LogisticRegression().fit(X_preds_log, y)\n",
    "print(\"Parameters of the second layer:\", f\"beta^T = [{logr3.intercept_[0]:.3f}, {logr3.coef_[0, 0]:.3f}, {logr3.coef_[0, 1]:.3f}]\")\n",
    "\n",
    "# for zero in-sample error\n",
    "# logr3.intercept_ = np.array([-1.55])\n",
    "# logr3.coef_ = np.array([[2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b7b4e-0dfa-4759-a423-55cda1252506",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "logr3_preds_grid = logr3.predict_proba(np.stack([logr1_preds_grid, logr2_preds_grid], axis=-1))[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax1, xx, yy, plot_min, plot_max, logr3_preds_grid)\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"Combined logistic regression model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23d4d2-a7a4-48f0-82fc-2b29fdf605ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "We can observe that the result is really similar to the combined linear classifier, but the predictions are now smooth.\n",
    "We still have to apply the threshold function at the end to obtain the final classes, but now we have sigmoid inside the model as a smooth alternative for the thresholding.\n",
    "\n",
    "This model architecture is now ready to be trained with gradient descent, but for that we will switch libraries.\n",
    "PyTorch (by Meta, as in Facebook) and the alternative TensorFlow (by Google) are the de facto standard libraries for neural networks in Python, which includes MLPs.\n",
    "This will also allow us to go further and implement a CNN next.\n",
    "We will use PyTorch.\n",
    "\n",
    "Have a look around the code cells for this example.\n",
    "They come with explainers of what they are about on a high level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddec845-d2cf-4b85-a025-806452bc0e2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "**1.** Imports and conversion\n",
    "\n",
    "`torch` brings general stuff like the `Tensor` object through the constructur `torch.tensor` and `torch.no_grad`, which simply disables PyTorch's automatic gradient calculation and therefore saves resources.\n",
    "You can think of a Tensor in PyTorch as a multidimensional array, like numpy arrays (`numpy.ndarray`).\n",
    "Unlike numpy arrays, Pytorch Tensors can be easily moved to a GPU to accelerate operations on them, if one is available.\n",
    "We convert our numpy arrays `X` and `y` to Tensors here.\n",
    "Also we use `torch.manual_seed` to predetermine the sequence of random numbers that are internally generated by PyTorch, wherever it uses randomness.\n",
    "This way everyone of us gets the same exact results.\n",
    "(Watch out that in practice, there can be sources of randomness in other libraries (such as numpy) that this doesn't control and that the random number generators in different versions of the same library (or dependencies) might yield different sequences of random numbers for the same seed.)\n",
    "\n",
    "`torch.nn` has a lot of functionality that is needed when working with neural networks.\n",
    "We will use `torch.nn.Linear` and `torch.nn.Sigmoid` as the building blocks for our MLP.\n",
    "We combine them into one complete model by defining a new class `XOR_MLP`, that inherits from `torch.nn.Module`.\n",
    "`torch.nn.MSELoss` supplies us with the optimization target we are used to.\n",
    "\n",
    "`torch.optim` contains implementations of various optimization algorithms.\n",
    "We use `torch.optim.SGD` (for stochastic gradient descent) here, which was presented in the lecture.\n",
    "\n",
    "`tqdm` enables us to show a progress bar for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4419c46-151d-4e6c-8d36-c876d7b368b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float64).reshape((len(y), 1))\n",
    "\n",
    "print(\"X Tensor:\", X_tensor.shape, X_tensor.dtype)\n",
    "print(\"y Tensor:\", y_tensor.shape, y_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1985c-3ef3-4a7b-a779-9f5dec491e9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "**2.** Model definition\n",
    "\n",
    "Here we define our MLP as a new class `XOR_MLP`.\n",
    "In `__init__` we define which building blocks our model should consist of and in `forward` we define how they should be wired up to form our model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c192cb-9e4d-411c-9f7c-9f4274f1f347",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2)  # first layer, representing our two linear models for feature extraction\n",
    "        self.output = nn.Linear(2, 1)  # second (and final) layer, representing the third linear model that combines the prediction of the first two logistic regression models\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()  # activation function, like in logistic regression\n",
    "\n",
    "        self.double()  # tell PyTorch to use parameters of dtype torch.float64, to match the inputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_out = self.sigmoid(self.hidden(x))  # wire up the first layer with sigmoid to use it as its activation function\n",
    "        output = self.sigmoid(self.output(hidden_out))  # wire up the second layer to use the first layer's predictions as input and use sigmoid as its activation function\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38b05d-64e3-4e1a-b9d3-8d79d59eede0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "**3.** Training\n",
    "\n",
    "Here we first initialize objects for the model, the loss function and the optimization algorithm.\n",
    "Then we use them in a training loop that implements mini-batch gradient descent.\n",
    "Note that in every epoch we first shuffle the data and then update the model parameters based on only `batch_size` many points.\n",
    "\n",
    "Updating the parameters consists of only a couple calls to PyTorch functions, which are very similar to the pseudocode from the lecture.\n",
    "The loss is first computed based on model predictions, then the gradients are computed and the gradient descent update rule $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L_B$ is applied, where $\\eta$ is the learning rate, which we supply to the SGD optimizer as the argument `lr` during instantiation.\n",
    "\n",
    "The call to `optimizer.zero_grad` is a pure technicality here, since PyTorch by default accumulates a buffer of past gradients, which we don't need and therefore clear using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c21a07-4f4d-4d34-91b2-6c6d83084a8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = XOR_MLP()\n",
    "criterion = nn.MSELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)  # SGD with learning rate 0.5\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "batch_size = 25\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Shuffle the data\n",
    "    indices = torch.randperm(len(X_tensor))\n",
    "    X_shuffled = X_tensor[indices]\n",
    "    y_shuffled = y_tensor[indices]\n",
    "    \n",
    "    # Process in mini-batches\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        # Get batch\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37761d95-00af-4c3f-b5c6-28dba83a341c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "**4.** Visualization of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e2009-1f67-476c-8fe3-fdc4e0aaf48b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "grid_points_tensor = torch.tensor(grid_points)\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(grid_points_tensor)\n",
    "    predicted_classes = predictions > 0.5\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plot_predictions_grid_regression(fig, ax1, xx, yy, plot_min, plot_max, predictions.numpy())\n",
    "plot_XOR_data(X, y, ax1)\n",
    "ax1.set_title(\"MLP predictions\")\n",
    "\n",
    "plot_predictions_grid_classification(fig, ax2, xx, yy, predicted_classes.numpy())\n",
    "plot_XOR_data(X, y, ax2)\n",
    "ax2.set_title(\"Thresholded MLP predictions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46956e-2f0b-4ebf-bbf3-892f403c4bc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "There is an interactive version of this model set up >[here](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,1&seed=0.27790&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)<.\n",
    "Go over there and click start to see how the model changes during training.\n",
    "Feel free to play around a bit by adding layers or neurons, changing the activation function or do whatever you like with the model/data configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39d0f7-7099-4f62-beb0-f0b99a9603c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "## Hands-on MNIST: MLP and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a9f17-8d55-4468-a233-0debb52a1722",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "Now that we have seen how to build an MLP in PyTorch, let's step it up a notch.\n",
    "We will take the MNIST dataset as an example image dataset.\n",
    "It consists of 28x28 pixel images of handwritten digits 0-9.\n",
    "You will create and train your own fully connected MLP and CNN to recognize the handwritten digits in the data.\n",
    "\n",
    "Let's first have a look at some example images from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e31a86-18b0-4361-9c60-f8c68b24ef03",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Function to plot one random image per class\n",
    "def plot_random_images_per_class(model, loader, device):\n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "    \n",
    "    # Dictionary to store one image per class\n",
    "    class_images = {}\n",
    "    class_labels = {}\n",
    "    \n",
    "    # Get one random image for each class\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        for i in range(len(target)):\n",
    "            label = target[i].item()\n",
    "            if label not in class_images:\n",
    "                class_images[label] = data[i].cpu().numpy()\n",
    "                class_labels[label] = target[i].item()\n",
    "    \n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 3))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            # Get the image and true label\n",
    "            img = class_images[i]\n",
    "            true_label = class_labels[i]\n",
    "            \n",
    "            # Convert from CHW to HWC for plotting\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            \n",
    "            # Normalize back to [0,1] range\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "            if model is not None:\n",
    "                # Get prediction\n",
    "                input_tensor = torch.tensor(img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "                output = model(input_tensor)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                pred_label = predicted.item()\n",
    "                axes[i].set_title(f'True: {true_label}, Pred: {pred_label}', \n",
    "                                  fontsize=10, color='blue' if true_label == pred_label else 'red')\n",
    "            else:\n",
    "                axes[i].set_title(f'Label: {true_label}')\n",
    "            \n",
    "            # Plot the image\n",
    "            axes[i].imshow(img, cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Random Image from Each MNIST Class', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f651-e4d1-476d-ae0b-8641114a3fec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL image to tensor (0-1 range), 3 channels\n",
    "    transforms.Normalize(mean=0.1307, std=0.3015),  # standardize data (mean 0, std 1)\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# plot\n",
    "plot_random_images_per_class(None, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea94991-3614-40e1-8ecd-c5f389f55434",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "As you can see, the images are in grayscale, so there is no color information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25711b92-e4cc-4faf-b2d7-3e504c44b3e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364569a9-ece4-44a3-ba5c-9a9626beac3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "If you struggle with MLPs, there are really good videos by 3Blue1Brown >[here](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)< (on MLPs / neural networks) and >[here](https://www.youtube.com/watch?v=IHZwWFHWa-w)< (on gradient descent).\n",
    "In case you want to watch them, please do so before or after the exercise session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124f1d9-247e-4453-bffc-20f51de102f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "1. What are the input dimensions ($H \\times W \\times C$)  for a single image? (**Hint**: $C$ stands for channels.)\n",
    "2. Implement a fully connected MLP (like the XOR MLP above) for classification of this dataset with 3 hidden layers (with 256, 128, 64 neurons respectively and ReLU activations) in the class `ImageMLP` below. (**Hint**: Take inspiration from the class `XOR_MLP` above. You can use the expression `x.view(x.size(0), -1)` to flatten a Tensor `x` in PyTorch.)\n",
    "3. How many trainable parameters are there in this model?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84060d3-cc62-4a57-ba0d-cc14aa161aee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "**1.** \n",
    "\n",
    "**3.** \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f82948-d4bc-4e2c-a37e-8be850e4d8be",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def get_loss(model, criterion, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "def show_next_plot(epochs, epoch_numbers, epoch_train_losses, epoch_test_losses):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.plot(epoch_numbers, epoch_train_losses, 'b-o', label='Train Loss', markersize=4)\n",
    "    ax1.plot(epoch_numbers, epoch_test_losses, 'r-o', label='Test Loss', markersize=4)\n",
    "    ax1.set_title('Loss over time')\n",
    "    ax1.set_xlabel('Epoch Number')\n",
    "    ax1.set_ylabel('Loss (Avg)')\n",
    "    ax1.set_xticks(range(epochs+1))\n",
    "    ax1.set_xlim(-0.5, epochs+0.5)\n",
    "    ax1.set_ylim(0, np.max([epoch_train_losses, epoch_test_losses])*1.1)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Update the figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_live_loss(model, criterion, optimizer, epochs=10):\n",
    "    print(\"Starting Training...\")\n",
    "    \n",
    "    # Set up live plotting\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "    \n",
    "    # Initialize lists for tracking\n",
    "    epoch_numbers = [0]\n",
    "    epoch_train_losses = [get_loss(model, criterion, train_loader)]\n",
    "    epoch_test_losses = [get_loss(model, criterion, test_loader)]\n",
    "\n",
    "    show_next_plot(epochs, epoch_numbers, epoch_train_losses, epoch_test_losses)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        # Save current epoch loss\n",
    "        epoch_numbers.append(epoch + 1)\n",
    "        epoch_train_losses.append(running_train_loss / len(train_dataset))\n",
    "        epoch_test_losses.append(get_loss(model, criterion, test_loader))\n",
    "        \n",
    "        # Clear and redraw the plot\n",
    "        clear_output(wait=True)\n",
    "        show_next_plot(epochs, epoch_numbers, epoch_train_losses, epoch_test_losses)\n",
    "    \n",
    "    # Disable interactive mode\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071f9f0-8234-4910-bfe3-80e984ab4d94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "class ImageMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Task 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Task 2\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab728c74-3611-4b86-9de5-8def0523f0cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "imagemlp_model = ImageMLP().to(device)  # send model weights to the GPU, if available\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # a popular choice for multi-class classification\n",
    "optimizer = optim.SGD(imagemlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# Start training\n",
    "train_live_loss(imagemlp_model, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855223f-abfb-4ebb-9c08-285fd7cee15d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_random_images_per_class(imagemlp_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eec472-f0b4-479f-83b0-fa6efa3424a4",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_targets), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a3525-3b6e-4847-a7d5-b716ff4a1296",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    print(\"Computing predictions...\")\n",
    "    true_labels, predicted_labels = get_model_predictions(model, test_loader, device)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title('Confusion Matrix for MNIST MLP')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "\n",
    "evaluate(imagemlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44411d8-2ffb-4764-b46f-333846f7813d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "heading"
    ]
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde6447-c20b-48a4-ba3c-6273bedc4f2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "explainer"
    ]
   },
   "source": [
    "There is an animated and interactive CNN visualization >[here](https://poloclub.github.io/cnn-explainer/)<, which can help you see how the core operations involved in CNNs operate on data.\n",
    "Make sure to scroll down and check out the section \"Understanding Hyperparameters\", if you have a hard time understanding kernel size, stride or padding in convolutions or pooling layers.\n",
    "\n",
    "There is also a great video by 3Blue1Brown >[here](https://www.youtube.com/watch?v=KuXjwB4LzSA)< about convolution, with a lot more visualizations and a really good explanation.\n",
    "In case you want to watch it, please do so before or after the exercise session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984b358-007f-4160-9d86-7aa3601aa0f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "tasks"
    ]
   },
   "source": [
    ":::{important} Tasks\n",
    "**1.** Implement a CNN for classification of the MNIST images with two convolutional blocks (each of the form `nn.Conv2d` $\\rightarrow$ `nn.ReLU` $\\rightarrow$ `nn.MaxPool2d`) followed by a fully connected hidden layer with ReLU activations in the class `CNN`. (**Hint**: You will again need to flatten images at some point, but not at the start this time.)\n",
    "\n",
    "- convolution configuration:\n",
    "  - number of feature maps (output channels): 8 for the first, 12 for the second\n",
    "  - kernel size: 3\n",
    "  - stride: 1\n",
    "  - padding: 1\n",
    "- pooling configuration:\n",
    "  - kernel size: 2\n",
    "  - stride: 2\n",
    "- fully connected hidden layer: 32 neurons\n",
    "\n",
    "**2.** What are the output dimensions ($H \\times W \\times C$) of the first full convolutional block (after the first pooling layer)?\n",
    "\n",
    "**3.** How many trainable parameters are there in this model?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a0166-82d6-49c4-9352-5e7a29363b2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    ":::{tip} Your answer\n",
    ":class:dropdown\n",
    "**2.** \n",
    "\n",
    "**3.** \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35413db-d7ac-4e22-8f8d-69ee5038ad32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Task 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Task 1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af536c6-a2a3-4a49-a29a-0b19581661f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "cnn_model = CNN().to(device)  # send model weights to the GPU, if available\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # a popular choice for multi-class classification\n",
    "optimizer = optim.SGD(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Start training\n",
    "train_live_loss(cnn_model, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273304b8-4507-4281-b268-048719b0d639",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "plot_random_images_per_class(cnn_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453fe08-6bf2-4524-92ad-da7352c94a9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "evaluate(cnn_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
