{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a247d44",
   "metadata": {},
   "source": [
    "---\n",
    "authors:\n",
    "  - name: Tom Siegl\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2dd2a2",
   "metadata": {},
   "source": [
    "# 03: Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a7f04",
   "metadata": {},
   "source": [
    "## Warmup: Curse of Dimensionality\n",
    "\n",
    "The uniform distribution $U[0,1]^d$ assigns equal probability density to vectors in the $d$-dimensional unit hypercube $[0,1]^d \\subset \\mathbb{R}^d$.\n",
    "\n",
    "The following code cell samples pairs of vectors from that distribution $10.000$ times and plots the distribution of the distances between these vector pairs in a histogram.\n",
    "There is one such plot for a couple of different choices for $d$.\n",
    "\n",
    "> Based on these plots, argue for the following two statements about the curse of dimensionality from the lecture.\n",
    "> 1. With increasing $d$, data become sparse.\n",
    "> 2. With increasing $d$, distances lose meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b2807",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877be1d9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x200 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dims = [1, 2, 5, 10, 50, 100, 1000]\n",
    "fig, axs = plt.subplots(1, len(dims), figsize=(2*len(dims), 2), sharey=\"row\")\n",
    "\n",
    "for i, (dim, ax) in enumerate(zip(dims, axs)):\n",
    "    num_iterations = 10000\n",
    "    dists = []\n",
    "\n",
    "    random_points1 = np.random.random((num_iterations, dim))\n",
    "    random_points2 = np.random.random((num_iterations, dim))\n",
    "    random_point_diffs = random_points1 - random_points2\n",
    "    dists = np.linalg.norm(random_point_diffs, axis=1)\n",
    "\n",
    "    ax.hist(dists, 100)\n",
    "    ax.set_xlim(0, dists.max())\n",
    "    ax.set_title(f\"d={dim}\")\n",
    "    ax.set_xlabel(\"distance\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979d7ee",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Here we will build PCA step by step.\n",
    "\n",
    "To have something to work with, we start by randomly generating some data points first.\n",
    "\n",
    "> 1. How many points are there in X?\n",
    "> 2. How many dimensions do the points in X have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5b851",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac4bdb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate 2D Gaussian blob\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=[2, 0],\n",
    "                                  cov=[[1, -1.5],\n",
    "                                       [-1.5, 3]],\n",
    "                                  size=10)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Original 2D Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5561c",
   "metadata": {},
   "source": [
    "The next cell just defines a function that lets us view the dimensionality-reducing projection of PCA, but doesn't apply it yet.\n",
    "\n",
    "> 3. Run the following cell. (Don't get stuck on reading the code here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156b7e1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def orthogonal_projection_1d(X, u):\n",
    "    pass\n",
    "\n",
    "\n",
    "def position_on_projection_vector(X_proj, u):\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_projection_2d(X, u, projection_func, ax):\n",
    "    # get projection\n",
    "    X_proj_1d = orthogonal_projection_1d(X, u)\n",
    "    X_proj = projection_func(X, u)\n",
    "\n",
    "    # show gridlines\n",
    "    ax.grid(zorder=0)\n",
    "\n",
    "    # show projection line\n",
    "    vec_line = np.array([\n",
    "        [u[0], -u[0]],  # xs\n",
    "        [u[1], -u[1]]   # ys\n",
    "    ]) * 1000\n",
    "    ax.plot(vec_line[0], vec_line[1], color=\"purple\", zorder=10)\n",
    "\n",
    "    for xi, xi_proj in zip(X, X_proj):\n",
    "        ax.plot([xi[0], xi_proj[0]], [xi[1], xi_proj[1]], color=(0, 0, 1), zorder=10)\n",
    "\n",
    "    # show data points\n",
    "    scatter_X = ax.scatter(X[:, 0], X[:, 1], color=(1, 0, 0), zorder=20)\n",
    "    scatter_X_proj = ax.scatter(X_proj[:, 0], X_proj[:, 1], color=(0, 1, 0), zorder=20)\n",
    "\n",
    "    # show projection vector\n",
    "    shift_magnitude = 0.1\n",
    "    vec_shift = np.array([-u[1], u[0]]) * shift_magnitude\n",
    "    ax.annotate(\"\", xytext=(vec_shift[0], vec_shift[1]), xy=(vec_shift[0]+u[0], vec_shift[1]+u[1]), arrowprops=dict(arrowstyle=\"->\"), zorder=30)\n",
    "    ax.text(vec_shift[0]+u[0], vec_shift[1]+u[1], r\"$\\mathbf{u}$\")\n",
    "    \n",
    "    # set axis limits\n",
    "    all_content = np.zeros((2*X.shape[0] + 1, 2))\n",
    "    all_content[:X.shape[0]] = X\n",
    "    all_content[X.shape[0]:2*X.shape[0]] = X_proj\n",
    "    all_content[-1] = u\n",
    "    xlim = [all_content[:, 0].min(), all_content[:, 0].max()]\n",
    "    ylim = [all_content[:, 1].min(), all_content[:, 1].max()]\n",
    "    padding = min([0.5, 0.1*(xlim[1]-xlim[0]), 0.1*(ylim[1]-ylim[0])])\n",
    "    xlim[0] -= padding\n",
    "    xlim[1] += padding\n",
    "    ylim[0] -= padding\n",
    "    ylim[1] += padding\n",
    "    xlim[0] = min(xlim[0], ylim[0])\n",
    "    xlim[1] = max(xlim[1], ylim[1])\n",
    "    ylim[0] = min(xlim[0], ylim[0])\n",
    "    ylim[1] = max(xlim[1], ylim[1])\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # define annotations\n",
    "    ax.set_title(r\"Projection on $u=\\binom{\"+f\"{u[0]:.2f}\"+r\"}{\"+f\"{u[1]:.2f}\"+r\"}$\"+\"\\n\"+r\"$Var(X_{proj})=$\"+f\"{X_proj_1d.var():.2f}\")\n",
    "    ax.set_xlabel(r\"$x_1$\")\n",
    "    ax.set_ylabel(r\"$x_2$\")\n",
    "    ax.legend([scatter_X, scatter_X_proj], [r\"$\\mathbf{x}_n$\", r\"$\\mathbf{x}_{n,proj}$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e59cb3",
   "metadata": {},
   "source": [
    "### Orthogonal Projection onto a Vector\n",
    "\n",
    "We're starting with just the projection of data points onto a vector.\n",
    "\n",
    "> 4. Implement the PCA projection from the lecture, which takes a data point $\\mathbf{x}_i$ and a vector $\\mathbf{u}$. (The notation is slightly different in the lecture slides.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd216f",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def orthogonal_projection_1d(X, u):\n",
    "    X_proj = []\n",
    "    for x_i in X:\n",
    "        X_proj.append(x_i)  # TODO: 4. replace x_i with its projection on u as taught in the lecture\n",
    "    return np.array(X_proj)\n",
    "\n",
    "\n",
    "u1 = np.array([1., 0.])\n",
    "u2 = np.array([1., 1.])\n",
    "u3 = np.array([2., 0.])\n",
    "X_proj1 = orthogonal_projection_1d(X, u1)\n",
    "X_proj2 = orthogonal_projection_1d(X, u2)\n",
    "X_proj3 = orthogonal_projection_1d(X, u3)\n",
    "\n",
    "print(f\"u1 = {u1} -> projection variance = {X_proj1.var()}\")\n",
    "print(f\"u2 = {u2} -> projection variance = {X_proj2.var()}\")\n",
    "print(f\"u3 = {u3} -> projection variance = {X_proj3.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf7bb7",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "Correct output:\n",
    "\n",
    "```raw\n",
    "u1 = [1. 0.] -> projection variance = 0.6023364389424803\n",
    "u2 = [1. 1.] -> projection variance = 0.42054013549497044\n",
    "u3 = [2. 0.] -> projection variance = 2.4093457557699214\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076f63e",
   "metadata": {},
   "source": [
    "The projected points are 1-dimensional now. The following code moves them back into 2D space and plots the projection. We do this by simply scaling $\\mathbf{u}$ by the projections per point.\n",
    "\n",
    "> 5. Run the following cell. (Don't get stuck on reading the code here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73537ae8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def scale_u_by_projection(X, u):\n",
    "    X_proj = orthogonal_projection_1d(X, u)\n",
    "    X_vecpos = []\n",
    "    for x_i in X_proj:\n",
    "        X_vecpos.append(x_i * u)\n",
    "    return np.array(X_vecpos)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 6))\n",
    "plot_projection_2d(X, u1, scale_u_by_projection, axs[0])\n",
    "plot_projection_2d(X, u2, scale_u_by_projection, axs[1])\n",
    "plot_projection_2d(X, u3, scale_u_by_projection, axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161f3f3",
   "metadata": {},
   "source": [
    "As you can see, this is not a proper orthogonal projection onto $\\mathbf{u}$. $\\mathbf{u}_1$ and $\\mathbf{u}_3$ should have the same results, because they are just scaled versions of each other.\n",
    "\n",
    "This problem really shows in the variances after the projection. (You can find them in the output of your code from task 4.)\n",
    "\n",
    "We could fix the projection for the general case by dividing the 1D projection values by the length of $\\mathbf{u}$.\n",
    "\n",
    "> 6. Why is this not a good solution, when we want to optimize $\\mathbf{u}$ for the variance of the 1D projection values?\n",
    "> 7. Which other solution does the lecture give for this problem? (Hint: What is $\\mathbf{u}^T \\mathbf{u}$ in the lecture?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025d9af",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "6. \n",
    "7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9da30d",
   "metadata": {},
   "source": [
    "### Choosing $\\mathbf{u}$\n",
    "\n",
    "Let's look at a set of choices for $\\mathbf{u}$, that meets the requirement from the lecture.\n",
    "\n",
    "The following code uniformly samples unit vectors from the unit circle between $0$ and $180$ degrees. Then the points are projected onto each one of them.\n",
    "\n",
    "> 8. Run the following cell. (Don't get stuck on reading the code here.)\n",
    "> 9. What would the projections onto unit vectors with angles between $180$ and $360$ degrees look like compared to those shown below?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a634245",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ee211",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_unit_vector_for_angle(theta):\n",
    "    return np.array([np.cos(theta), np.sin(theta)])\n",
    "\n",
    "\n",
    "# sample unit vectors with different rotations\n",
    "thetas8 = np.linspace(0, np.pi, 8)\n",
    "\n",
    "# plot sampled vectors\n",
    "fig, axs = plt.subplots(1, 8, figsize=(18,2), sharey=\"row\")\n",
    "for theta, ax in zip(thetas8, axs):\n",
    "    theta_vec = get_unit_vector_for_angle(theta)\n",
    "    ax.annotate(\"\", xytext=(0,0), xy=(theta_vec[0], theta_vec[1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_title(r\"$\\mathbf{u}\"+r\"=\\binom{\"+f\"{theta_vec[0]:.2f}\"+r\"}{\"+f\"{theta_vec[1]:.2f}\"+r\"}$\"+f\"\\n $\\\\theta$={np.degrees(theta):.2f} deg\")\n",
    "plt.show()\n",
    "\n",
    "# plot projections\n",
    "fig1, axs1 = plt.subplots(1, 4, figsize=(20,4))\n",
    "fig2, axs2 = plt.subplots(1, 4, figsize=(20,4))\n",
    "\n",
    "for theta, ax in zip(thetas8, [*axs1, *axs2]):\n",
    "    theta_vec = get_unit_vector_for_angle(theta)\n",
    "    plot_projection_2d(X, theta_vec, scale_u_by_projection, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d27d94",
   "metadata": {},
   "source": [
    "All of these vectors are candidates for our principle components, but which to choose?\n",
    "\n",
    "We want to maximize the variance after the projection, so let's have a look at how the variance responds to differently rotated unit vectors.\n",
    "\n",
    "> 10. Run the following cell. (Don't get stuck on reading the code here.)\n",
    "\n",
    "Great! We can see in which direction $\\mathbf{u}$ should point to maximize the variance after the projection. So we could just pick this as our solution. But this is not the algorithm taught in the lecture...\n",
    "\n",
    "> 11. Name two shortcomings of this algorithm to finding principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6269ea7",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbb6b1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "thetas_many = np.linspace(0, np.pi, 180)\n",
    "thetas_many_vars = [np.var(orthogonal_projection_1d(X, get_unit_vector_for_angle(t))) for t in thetas_many]\n",
    "\n",
    "plt.plot(np.degrees(thetas_many), thetas_many_vars)\n",
    "plt.xlabel('Projection Angle (degrees)')\n",
    "plt.ylabel('Projected Variance')\n",
    "plt.title('Variance Along Different Directions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a4ea4",
   "metadata": {},
   "source": [
    "### Linear Algebra of Covariances\n",
    "\n",
    "Let's speed this up!\n",
    "\n",
    "To get there, let's first have a look at the covariance matrix of our data and what it means.\n",
    "\n",
    "To illustrate, we start by manipulating our data to have unit variance, i.e. such that the covariance matrix is a unit/identity matrix (called \"whitening\").\n",
    "\n",
    "Afterwards is where the interesting part will happen, which is why your task again is to simply:\n",
    "\n",
    "> 10. Run the following cell. (Don't get stuck on reading the code here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bdb64",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X_covariance = np.cov(X, rowvar=False)\n",
    "X_eigvals, X_eigvecs = np.linalg.eig(X_covariance)\n",
    "X_whitening_transform = X_eigvecs @ np.diag(1.0 / np.sqrt(X_eigvals)) @ X_eigvecs.T\n",
    "\n",
    "X_whitened = (X - X.mean()) @ X_whitening_transform\n",
    "\n",
    "print(\"Covariance matrix of whitened data:\")\n",
    "print(np.cov(X_whitened, rowvar=False))\n",
    "\n",
    "plt.scatter(X_whitened[:, 0], X_whitened[:, 1])\n",
    "plt.title(\"Whitened X\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229d956",
   "metadata": {},
   "source": [
    "The whitened points are now evenly distributed around the center of the coordinate system, with no dominant direction.\n",
    "\n",
    "Next we will see how the covariance matrix acts on these points $W$ to obtain our original unevenly distributed $X$.\n",
    "\n",
    "> 11. Run the following cell. (Don't get stuck on reading the code here.)\n",
    "\n",
    "The resulting plots show the transformation $W$ to $X$ step by step.\n",
    "\n",
    "Notation:\n",
    "\n",
    "Eigenvectors: $q_1 = \\binom{q_{1,1}}{q_{1,2}}$ and $q_2 = \\binom{q_{2,1}}{q_{2,2}}$, Eigenvalues: $\\lambda_1$ and $\\lambda_2$\n",
    "\n",
    "Matrices of Eigenvectors and -values: \n",
    "$Q = \\begin{bmatrix}\n",
    "q_{1,1} & q_{2,1} \\\\\n",
    "q_{1,2} & q_{2,2}\n",
    "\\end{bmatrix}$\n",
    ",\n",
    "$\\Lambda = diag(\\lambda_1, \\lambda_2) =\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0\\\\\n",
    "0 & \\lambda_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Whitened data:\n",
    "$W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{2,1} & \\cdots w_{n,1}\\\\\n",
    "w_{1,2} & w_{2,2} & \\cdots w_{n,2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "> 12. What are the steps taken from one plot to the next? Assign these names of transformations to the plot numbers: \"translation\", \"scaling\", \"rotation\", \"shearing\" (Repetitions and omissions are allowed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89e8ab",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "12.\n",
    "\n",
    "(1): no transformation yet, this is the starting point\n",
    "\n",
    "(2): \n",
    "\n",
    "(3): \n",
    "\n",
    "(4): \n",
    "\n",
    "(5): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318072f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X_covariance = np.cov(X, rowvar=False)\n",
    "X_eigvals, X_eigvecs = np.linalg.eig(X_covariance)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "axs[0].scatter(X_whitened[:, 0], X_whitened[:, 1])\n",
    "axs[0].annotate(\"\", xytext=(0,0), xy=(X_eigvecs[0, 0], X_eigvecs[0, 1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[0].annotate(\"\", xytext=(0,0), xy=(X_eigvecs[1, 0], X_eigvecs[1, 1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[0].set_title(\"(1) $W$\")\n",
    "axs[0].set_xlabel(\"$x_1$\")\n",
    "axs[0].set_ylabel(\"$x_2$\")\n",
    "axs[0].set_xlim(-2.5, 2.5)\n",
    "axs[0].set_ylim(-2.5, 2.5)\n",
    "\n",
    "X_whitened_in_eigvec_space = X_eigvecs.T @ X_whitened.T\n",
    "X_eigvecs_in_eigvec_space = X_eigvecs.T @ X_eigvecs\n",
    "\n",
    "axs[1].scatter(X_whitened_in_eigvec_space[0, :], X_whitened_in_eigvec_space[1, :])\n",
    "axs[1].annotate(\"\", xytext=(0,0), xy=(X_eigvecs_in_eigvec_space[0, 0], X_eigvecs_in_eigvec_space[1, 0]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[1].annotate(\"\", xytext=(0,0), xy=(X_eigvecs_in_eigvec_space[0, 1], X_eigvecs_in_eigvec_space[1, 1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[1].set_title(\"(2) $Q^TW$\")\n",
    "axs[1].set_xlabel(\"$x_1$\")\n",
    "axs[1].set_ylabel(\"$x_2$\")\n",
    "axs[1].set_xlim(-2.5, 2.5)\n",
    "axs[1].set_ylim(-2.5, 2.5)\n",
    "\n",
    "X_unwhitened_in_eigvec_space = np.diag(np.sqrt(X_eigvals)) @ X_whitened_in_eigvec_space\n",
    "X_scaled_eigvecs_in_eigvec_space = np.diag(np.sqrt(X_eigvals)) @ X_eigvecs_in_eigvec_space\n",
    "\n",
    "axs[2].scatter(X_unwhitened_in_eigvec_space[0, :], X_unwhitened_in_eigvec_space[1, :])\n",
    "axs[2].annotate(\"\", xytext=(0,0), xy=(X_scaled_eigvecs_in_eigvec_space[0, 0], X_scaled_eigvecs_in_eigvec_space[1, 0]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[2].annotate(\"\", xytext=(0,0), xy=(X_scaled_eigvecs_in_eigvec_space[0, 1], X_scaled_eigvecs_in_eigvec_space[1, 1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[2].set_title(r\"(3) $\\Lambda^{1/2}Q^TW$\")\n",
    "axs[2].set_xlabel(\"$x_1$\")\n",
    "axs[2].set_ylabel(\"$x_2$\")\n",
    "axs[2].set_xlim(-2.5, 2.5)\n",
    "axs[2].set_ylim(-2.5, 2.5)\n",
    "\n",
    "X_unwhitened = X_eigvecs @ X_unwhitened_in_eigvec_space\n",
    "X_scaled_eigvecs = X_eigvecs @ X_scaled_eigvecs_in_eigvec_space\n",
    "\n",
    "axs[3].scatter(X_unwhitened[0, :], X_unwhitened[1, :])\n",
    "axs[3].annotate(\"\", xytext=(0,0), xy=(X_scaled_eigvecs[0, 0], X_scaled_eigvecs[1, 0]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[3].annotate(\"\", xytext=(0,0), xy=(X_scaled_eigvecs[0, 1], X_scaled_eigvecs[1, 1]), arrowprops=dict(arrowstyle=\"->\"))\n",
    "axs[3].set_title(r\"(4) $Q\\Lambda^{1/2}Q^TW$\")\n",
    "axs[3].set_xlabel(\"$x_1$\")\n",
    "axs[3].set_ylabel(\"$x_2$\")\n",
    "axs[3].set_xlim(-2.5, 2.5)\n",
    "axs[3].set_ylim(-2.5, 2.5)\n",
    "\n",
    "X_reconstructed = X_unwhitened + X.mean()\n",
    "\n",
    "axs[4].scatter(X[:, 0], X[:, 1], s=80)\n",
    "axs[4].scatter(X_reconstructed[0, :], X_reconstructed[1, :])\n",
    "axs[4].set_title(\"(5) Reconstructed X on original X\")\n",
    "axs[4].set_xlabel(\"$x_1$\")\n",
    "axs[4].set_ylabel(\"$x_2$\")\n",
    "axs[4].legend([\"Original\", \"Reconstructed\"])\n",
    "\n",
    "# print(\"Covariance matrix of original X:\")\n",
    "# print(np.cov(X, rowvar=False))\n",
    "# print(\"Covariance matrix of reconstructed X:\")\n",
    "# print(np.cov(X_reconstructed.T, rowvar=False))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cc763",
   "metadata": {},
   "source": [
    "Maybe at this point you ask \"Why does this have to be so complicated?\" and \"Can't we just multiply our whitened points $W$ with the covariance matrix of $X$?\". After all, the Eigenvectors of a transformation matrix are supposed to be equal to the directions in which the matrix only stretches points and the Eigenvectors are supposed to be the magnitude of that stretch.\n",
    "\n",
    "You can have a quick look at the output of the following cell to check that idea. It's not too far off, it's just that the scaling is not on point. This is why we have to take the square root of the Eigenvalues for a correct reconstruction.\n",
    "\n",
    "> 13. Run the following cell. (Don't get stuck on reading the code here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86a66c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Eigenvalues: {X_eigvals}\")\n",
    "\n",
    "X_reconstructed_naive = (np.cov(X, rowvar=False) @ X_whitened.T) + X.mean()\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(X_reconstructed_naive[0, :], X_reconstructed_naive[1, :])\n",
    "plt.title(\"Naively reconstructed X on original X\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.legend([\"Original\", \"Reconstructed\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcfdbd",
   "metadata": {},
   "source": [
    "### Explicit Covariance-based Implementation\n",
    "\n",
    "The next code cell has an implementation of PCA based on the eigendecomposition of the covariance matrix of the data.\n",
    "\n",
    "> 14. Why don't we take the square root of the Eigenvalues here?\n",
    "> 15. Why does the `pca_cov` function sort the Eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6681b",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "14. \n",
    "15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_cov(X):\n",
    "    C = np.cov(X, rowvar=False)  # Covariance matrix\n",
    "    vals, vecs = np.linalg.eig(C)  # Eigen decomposition of covariance matrix\n",
    "\n",
    "    # Sort eigenvalues\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    vals = vals[idx]\n",
    "    vecs = vecs[:, idx]\n",
    "\n",
    "    return vecs\n",
    "\n",
    "\n",
    "# don't get stuck reading the plotting part below\n",
    "us_pca = pca_cov(X)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(us_pca[0]), figsize=(len(us_pca[0])*7, 6))\n",
    "for dim in range(len(us_pca[0])):\n",
    "    plot_projection_2d(X, us_pca[:, dim], scale_u_by_projection, axs[dim])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fe361",
   "metadata": {},
   "source": [
    "### SVD-based Implementation\n",
    "\n",
    "Computing the entire covariance matrix in high dimensions can get inefficient and is not necessary.\n",
    "\n",
    "The singular value decomposition is a \"shortcut\" we can take to obtain the Eigenvectors of the covariance matrix without explicit computation of the covariance matrix.\n",
    "\n",
    "> 16. The SVD-based PCA needs to center the data first. Why is this not needed in the last version, that explicitly computes the covariances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db8e73",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b741426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_svd(X):\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    _, _, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    return Vt\n",
    "\n",
    "\n",
    "# don't get stuck reading the plotting part below\n",
    "us_pca = pca_svd(X)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(us_pca[0]), figsize=(len(us_pca[0])*7, 6))\n",
    "for dim in range(len(us_pca[0])):\n",
    "    plot_projection_2d(X, us_pca[:, dim], scale_u_by_projection, axs[dim])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92848fd4",
   "metadata": {},
   "source": [
    "### Fraction of Explained Variance\n",
    "\n",
    "> 17. How can the fraction of explained variance be used to determine how many principal components we should take from a given dataset, in order to preserve the bulk of the contained information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb57370",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "17. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941f5c3",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f03921",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "> 1. Is clustering a supervised or an unsupervised learning problem?\n",
    "> 2. Improve the points in the `centroids` variable below, such that the clusters visually look like they capture the data better to you! Give it just one shot, no need for perfection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf8975",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c486a3",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=0)\n",
    "\n",
    "centroids = np.array([  # TODO: 2. these points seem a little off. Can you improve them?\n",
    "    [-1.5, 6],\n",
    "    [1.5, 2],\n",
    "    [-1.5, 2]\n",
    "])\n",
    "labels = np.argmin(((X_blobs[:, None, :] - centroids[None, :, :])**2).sum(axis=2), axis=1)\n",
    "\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "plt.title(\"Data Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e9b86",
   "metadata": {},
   "source": [
    "### Optimization Target\n",
    "\n",
    "Let's have a look at whether your new cluster definition is better that the one given. For that, we will have to define what makes a good clustering.\n",
    "\n",
    "> 3. Implement the target function from the lecture in `clustering_cost`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7762d23",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def clustering_cost(X, centroids, labels):\n",
    "    pass  # TODO: 3. implement\n",
    "\n",
    "# Given centroids from the previous task\n",
    "centroids_1 = np.array([[-1.5, 4], [1.5, 2], [-1.5, 2]])\n",
    "labels_1 = np.argmin(((X_blobs[:, None, :] - centroids_1[None, :, :])**2).sum(axis=2), axis=1)\n",
    "\n",
    "cost_1 = clustering_cost(X_blobs, centroids_1, labels_1)\n",
    "print(f\"Cost of given centroids: {cost_1:.2f}\")\n",
    "cost = clustering_cost(X_blobs, centroids, labels)\n",
    "print(f\"Cost of improved centroids: {cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa83150",
   "metadata": {},
   "source": [
    "> 4. The defined points are called `centroids` in the code, but that is not correct. Why are they not actual centroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a46821",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae9bf2",
   "metadata": {},
   "source": [
    "### Lloyd's Algorithm\n",
    "\n",
    "> 5. How are the clusters initialized in the following implementation of k-means? Was it the same in the lecture?\n",
    "> 6. Look up the k-means documentation of sklearn. What other options are there for initialization?\n",
    "> 7. Complete the following implementation of the k-means clustering algorithm from the lecture! You just need to add a single function call for each of the question marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d9e86",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "5. \n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ca8bd",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def kmeans(X, k=3, n_iter=10):\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        labels = ?(((X[:, None, :] - centroids[None, :, :])**2).sum(axis=2), axis=1)  # TODO: 7. complete (insert a function) at the question mark\n",
    "        centroids = np.array([X[labels == j].? for j in range(k)])  # TODO: 7. complete (insert a function) at the question mark\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "centroids, labels = kmeans(X_blobs, k=3, n_iter=10)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "plt.title(\"k-Means Result\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-ws-25-26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
