{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2dd2a2",
   "metadata": {},
   "source": [
    "# 03: Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf291773",
   "metadata": {},
   "source": [
    "## k-means Clustering\n",
    "\n",
    "1. What is a cluster in data? Discuss with your neighbours!\n",
    "2. Is clustering a supervised or an unsupervised learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d6949",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8660a",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "> 1. Look at the plot below. What does this mean for the pairwise distances between the points in high dimensions?\n",
    "> 2. Why does this make the analysis of high-dimensional data hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d872823",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def min_max_ratio(points):\n",
    "    # Compute pairwise distances\n",
    "    dists = np.array([np.linalg.norm(p1 - p2) for p1, p2 in itertools.combinations(points, 2)])\n",
    "    print(dists.min(), dists.max())\n",
    "    return dists.min() / dists.max()\n",
    "\n",
    "dims = [2, 5, 10, 20, 50, 100]\n",
    "ratios = []\n",
    "\n",
    "for d in dims:\n",
    "    points = np.random.rand(1000, d)\n",
    "    ratios.append(min_max_ratio(points))\n",
    "\n",
    "plt.plot(dims, ratios, marker='o')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Min/Max Distance Ratio')\n",
    "plt.title('Curse of Dimensionality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979d7ee",
   "metadata": {},
   "source": [
    "### Variance as Information\n",
    "\n",
    "We randomly generate some data points first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D Gaussian blob\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=[0, 0],\n",
    "                                  cov=[[-1, 1.5],\n",
    "                                       [1.5, -3]],\n",
    "                                  size=500)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Original 2D Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e87362",
   "metadata": {},
   "source": [
    "> 3. Complete the code of the projection function below!\n",
    "> 4. Plot the projected points for a couple of different angles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94367699",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute variance along a projection axis\n",
    "def projected_variance(X, theta):\n",
    "    # Projection direction (unit vector) rotated by theta\n",
    "    w = np.array([np.cos(theta), np.sin(theta)])\n",
    "    X_proj = ?  # TODO: 3. fill in implementation where the question mark is\n",
    "    return X_proj.var()\n",
    "\n",
    "thetas = np.linspace(0, np.pi, 180)\n",
    "vars_ = [projected_variance(X, t) for t in thetas]\n",
    "\n",
    "plt.plot(np.degrees(thetas), vars_)\n",
    "plt.xlabel('Projection Angle (degrees)')\n",
    "plt.ylabel('Projected Variance')\n",
    "plt.title('Variance Along Different Directions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1865488",
   "metadata": {},
   "source": [
    "> 5. What is preferable about projections with higher variance? (E.g. for learning tasks or data visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93f0d4",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429791d",
   "metadata": {},
   "source": [
    "### Building PCA\n",
    "\n",
    "> 6. Figure out what to do to the Eigenvector / Eigenvalue lists to complete the PCA implementation below and implement that change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9de84",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Center data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Covariance matrix\n",
    "C = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Eigen decomposition\n",
    "vals, vecs = eig(C)\n",
    "\n",
    "# TODO: 6. what to do with the vals and vecs?\n",
    "\n",
    "# Project onto first component\n",
    "X_pca_manual = X_centered @ vecs[:, :1]\n",
    "\n",
    "# Compare with sklearn PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_pca_sklearn = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_pca_manual, np.zeros_like(X_pca_manual), label='Manual PCA', alpha=0.6)\n",
    "plt.scatter(X_pca_sklearn, np.zeros_like(X_pca_sklearn)-0.1, label='sklearn PCA', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.title('Manual vs sklearn PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12159a9",
   "metadata": {},
   "source": [
    "> 7. Reconstruct 2D points from the PCA-transformed points! (Hint: Check out the sklearn documentation for PCA.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fdc3b",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "pca_recon = PCA(n_components=1)\n",
    "X_reduced = pca_recon.fit_transform(X)\n",
    "X_reconstructed = ?  # TODO: 7. complete at the question mark\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label='Original')\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.6, label='Reconstructed')\n",
    "plt.legend()\n",
    "plt.title(\"PCA Reconstruction (1 Component)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941f5c3",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f03921",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "> 1. Is clustering a supervised or an unsupervised learning problem?\n",
    "> 2. Improve the points in the `centroids` variable below, such that the clusters visually look like they capture the data better to you! Give it just one shot, no need for perfection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c486a3",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=0)\n",
    "\n",
    "centroids = np.array([  # TODO: 2. these points seem a little off. Can you improve them?\n",
    "    [-1.5, 4],\n",
    "    [1.5, 2],\n",
    "    [-1.5, 2]\n",
    "])\n",
    "labels = np.argmin(((X_blobs[:, None, :] - centroids[None, :, :])**2).sum(axis=2), axis=1)\n",
    "\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "plt.title(\"Data Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e9b86",
   "metadata": {},
   "source": [
    "### Optimization Target\n",
    "\n",
    "Let's have a look at whether your new cluster definition is better that the one given. For that, we will have to define what makes a good clustering.\n",
    "\n",
    "> 3. Implement the target function from the lecture in `clustering_cost`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7762d23",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def clustering_cost(X, centroids, labels):\n",
    "    pass  # TODO: 3. implement\n",
    "\n",
    "# Given centroids from the previous task\n",
    "centroids_1 = np.array([[-1.5, 4], [1.5, 2], [-1.5, 2]])\n",
    "labels_1 = np.argmin(((X_blobs[:, None, :] - centroids_1[None, :, :])**2).sum(axis=2), axis=1)\n",
    "\n",
    "cost_1 = clustering_cost(X_blobs, centroids_1, labels_1)\n",
    "print(f\"Cost of given centroids: {cost_1:.2f}\")\n",
    "cost = clustering_cost(X_blobs, centroids, labels)\n",
    "print(f\"Cost of improved centroids: {cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa83150",
   "metadata": {},
   "source": [
    "> 4. The defined points are called `centroids` in the code, but that is not correct. Why are they not actual centroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a46821",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae9bf2",
   "metadata": {},
   "source": [
    "### Lloyd's Algorithm\n",
    "\n",
    "> 5. How are the clusters initialized in the following implementation of k-means? Was it the same in the lecture?\n",
    "> 6. Look up the k-means documentation of sklearn. What other options are there for initialization?\n",
    "> 7. Complete the following implementation of the k-means clustering algorithm from the lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d9e86",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "5. \n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ca8bd",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "outputs": [],
   "source": [
    "def kmeans(X, k=3, n_iter=10):\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        labels = ?(((X[:, None, :] - centroids[None, :, :])**2).sum(axis=2), axis=1)  # TODO: 7. complete (insert a function) at the question mark\n",
    "        centroids = np.array([X[labels == j].? for j in range(k)])  # TODO: 7. complete (insert a function) at the question mark\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "centroids, labels = kmeans(X_blobs, k=3, n_iter=10)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "plt.title(\"k-Means Result\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c4c52",
   "metadata": {},
   "source": [
    "### Connection to PCA\n",
    "\n",
    "> 7. Run the following code cells a couple of times. What do you notice in the clusters, in case two centroids share on of the blobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ee100",
   "metadata": {
    "tags": [
     "placeholder"
    ]
   },
   "source": [
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950425fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high, _ = make_blobs(n_samples=500, centers=3, n_features=50, random_state=42)\n",
    "centroids_high, labels_high = kmeans(X_high, k=3)\n",
    "X_vis = PCA(n_components=2).fit_transform(X_high)\n",
    "\n",
    "plt.scatter(X_vis[:,0], X_vis[:,1], c=labels_high, cmap='rainbow', alpha=0.7)\n",
    "plt.title(\"k-Means on 50D Data (Visualized in 2D)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vis = PCA(n_components=2).fit_transform(X_high)\n",
    "\n",
    "centroids_low, labels_low = kmeans(X_vis, k=3)\n",
    "plt.scatter(X_vis[:,0], X_vis[:,1], c=labels_low, cmap='rainbow', alpha=0.7)\n",
    "plt.title(\"k-Means on 50D Data (Visualized in 2D)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-ws-25-26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
